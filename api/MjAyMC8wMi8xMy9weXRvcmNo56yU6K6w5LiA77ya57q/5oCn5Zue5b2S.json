{"title":"pytorch笔记一：线性回归","date":"2020-02-13T07:24:54.000Z","date_formatted":{"ll":"Feb 13, 2020","L":"02/13/2020","MM-DD":"02-13"},"thumbnail":"https://s2.ax1x.com/2020/02/13/1LCri4.png","link":"2020/02/13/pytorch笔记一：线性回归","comments":true,"tags":["pytorch"],"categories":["Notes"],"updated":"2020-03-07T06:40:20.213Z","content":"<blockquote>\n<p>资料链接：<a href=\"https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/docs/chapter03_DL-basics/3.1_linear-regression.md\" target=\"_blank\">https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/docs/chapter03_DL-basics/3.1_linear-regression.md</a></p>\n</blockquote>\n<p>在开始线性回归之前还是先看一下基本数据操作。</p>\n<h3 id=\"数据操作\">数据操作<a href=\"#数据操作\" title=\"数据操作\"></a></h3><h4 id=\"1-创建tensor\">1. 创建Tensor<a href=\"#1-创建tensor\" title=\"1. 创建Tensor\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#(1) 创建一个 5x3 的未初始化的 Tensor</span></span><br><span class=\"line\">x = torch.empty(<span class=\"number\">5</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"comment\">#(2) 创建一个 5x3 的随机初始化的 Tensor</span></span><br><span class=\"line\">x = torch.rand(<span class=\"number\">5</span>, <span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"comment\">#(3) 创建一个 5x3 的 long 型全 0 的 Tensor</span></span><br><span class=\"line\">x = torch.zeros(<span class=\"number\">5</span>, <span class=\"number\">3</span>, dtype=torch.long)</span><br><span class=\"line\"><span class=\"comment\">#(4) 直接根据数据创建</span></span><br><span class=\"line\">x = torch.tensor([<span class=\"number\">5.5</span>,<span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure>\n\n<p>还可以通过现有的 <code>Tensor</code> 来创建，此方法会默认重用输入 <code>Tensor</code> 的一些属性，例如数据类型，除非自定义数据类型。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">5</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\">x = x.new_ones(<span class=\"number\">5</span>, <span class=\"number\">3</span>)  <span class=\"comment\"># 返回的tensor默认具有相同的torch.dtype和torch.device</span></span><br><span class=\"line\">print(x)</span><br><span class=\"line\"></span><br><span class=\"line\">x = torch.ones_like(x, dtype=torch.float) <span class=\"comment\"># 指定新的数据类型</span></span><br><span class=\"line\">print(x)</span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[1, 1, 1],</span><br><span class=\"line\">        [1, 1, 1],</span><br><span class=\"line\">        [1, 1, 1],</span><br><span class=\"line\">        [1, 1, 1],</span><br><span class=\"line\">        [1, 1, 1]])</span><br><span class=\"line\">tensor([[1., 1., 1.],</span><br><span class=\"line\">        [1., 1., 1.],</span><br><span class=\"line\">        [1., 1., 1.],</span><br><span class=\"line\">        [1., 1., 1.],</span><br><span class=\"line\">        [1., 1., 1.]])</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(x.size())</span><br><span class=\"line\">print(x.shape) <span class=\"comment\">#获取 Tensor 的形状</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"2-算术操作\">2. 算术操作<a href=\"#2-算术操作\" title=\"2. 算术操作\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.rand(<span class=\"number\">5</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">y = torch.rand(<span class=\"number\">5</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\"><span class=\"comment\">#加法形式一</span></span><br><span class=\"line\">print(x+y)</span><br><span class=\"line\"><span class=\"comment\">#加法形式二</span></span><br><span class=\"line\">print(torch.add(x, y))</span><br><span class=\"line\">\t<span class=\"comment\">#可指定result</span></span><br><span class=\"line\">    result = torch.empty(<span class=\"number\">5</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">    torch.add(x,y,out=result)</span><br><span class=\"line\">    print(result)</span><br><span class=\"line\"><span class=\"comment\">#加法形式三</span></span><br><span class=\"line\">y.add_(x)</span><br><span class=\"line\">print(y)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"3-改变形状\">3. 改变形状<a href=\"#3-改变形状\" title=\"3. 改变形状\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#用 view() 来改变 Tensor 的形状：</span></span><br><span class=\"line\">x = torch.rand(<span class=\"number\">5</span>,<span class=\"number\">3</span>)</span><br><span class=\"line\">y = x.view(<span class=\"number\">15</span>)\t<span class=\"comment\">#y是1*15</span></span><br><span class=\"line\">z = x.view(<span class=\"number\">-1</span>, <span class=\"number\">5</span>)  <span class=\"comment\"># -1所指的维度可以根据其他维度的值推出来，z是3*5</span></span><br></pre></td></tr></table></figure>\n\n<p>注意 <code>view()</code> 返回的新 <code>Tensor</code> 与源 <code>Tensor</code> 虽然可能有不同的 <code>size</code>，但是是共享 <code>data</code> 的，也即现在更改x的值，y也会跟着改变。</p>\n<p>所以如果我们想返回一个真正新的副本（即不共享 data 内存），推荐先用 <code>clone</code> 创造一个副本然后再使用 <code>view</code>。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x_cp = x.clone().view(<span class=\"number\">15</span>)</span><br><span class=\"line\">x -= <span class=\"number\">1</span>\t<span class=\"comment\">#x的元素值减1，x_cp不变</span></span><br></pre></td></tr></table></figure>\n\n<p>使用 <code>clone</code> 还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源 <code>Tensor</code>。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#iter()函数将一个标量 Tensor 转换成一个 Python number</span></span><br><span class=\"line\">x = torch.randn(<span class=\"number\">1</span>)</span><br><span class=\"line\">print(x)\t<span class=\"comment\">#输出tensor([2.3466])</span></span><br><span class=\"line\">print(x.item())\t<span class=\"comment\">#输出2.3466382026672363</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"4-广播机制\">4. 广播机制<a href=\"#4-广播机制\" title=\"4. 广播机制\"></a></h4><p><a href=\"https://pytorch.org/docs/stable/notes/broadcasting.html\" target=\"_blank\">https://pytorch.org/docs/stable/notes/broadcasting.html</a></p>\n<p>当对两个形状不同的 <code>Tensor</code> 按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个 <code>Tensor</code> 形状相同后再按元素运算。</p>\n<h4 id=\"5-tensor-和-numpy-相互转换\">5. <code>Tensor</code> 和 NumPy 相互转换<a href=\"#5-tensor-和-numpy-相互转换\" title=\"5. Tensor 和 NumPy 相互转换\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 使用 numpy() 将 Tensor 转换成 NumPy 数组</span></span><br><span class=\"line\"><span class=\"comment\"># 产生的 Tensor 和 NumPy 中的数组共享相同的内存,改变其中一个时另一个也会改变</span></span><br><span class=\"line\">a = torch.ones(<span class=\"number\">5</span>)</span><br><span class=\"line\">b = a.numpy()</span><br><span class=\"line\">print(a, b)\t<span class=\"comment\"># tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]</span></span><br><span class=\"line\"></span><br><span class=\"line\">a += <span class=\"number\">1</span></span><br><span class=\"line\">print(a, b)\t<span class=\"comment\"># tensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]</span></span><br><span class=\"line\">b += <span class=\"number\">1</span></span><br><span class=\"line\">print(a, b)\t<span class=\"comment\"># tensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 使用 from_numpy() 将 NumPy 数组转换成 Tensor</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">a = np.ones(<span class=\"number\">5</span>)</span><br><span class=\"line\">b = torch.from_numpy(a)</span><br><span class=\"line\">print(a, b)\t<span class=\"comment\"># [1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)</span></span><br><span class=\"line\"></span><br><span class=\"line\">a += <span class=\"number\">1</span></span><br><span class=\"line\">print(a, b)\t<span class=\"comment\"># [2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)</span></span><br><span class=\"line\">b += <span class=\"number\">1</span></span><br><span class=\"line\">print(a, b)\t<span class=\"comment\"># [3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 不共享内存</span></span><br><span class=\"line\">c = torch.tensor(a)</span><br><span class=\"line\">a += <span class=\"number\">1</span></span><br><span class=\"line\">print(a, c)\t<span class=\"comment\"># [4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"6-tensor-on-gpu\">6. <code>Tensor</code> on GPU<a href=\"#6-tensor-on-gpu\" title=\"6. Tensor on GPU\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 用方法 to() 可以将 Tensor 在 CPU 和 GPU（需要硬件支持）之间相互移动</span></span><br><span class=\"line\"><span class=\"comment\"># 以下代码只有在PyTorch GPU版本上才会执行</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> torch.cuda.is_available():</span><br><span class=\"line\">    device = torch.device(<span class=\"string\">\"cuda\"</span>)          <span class=\"comment\"># GPU</span></span><br><span class=\"line\">    y = torch.ones_like(x, device=device)  <span class=\"comment\"># 直接创建一个在GPU上的Tensor</span></span><br><span class=\"line\">    x = x.to(device)                       <span class=\"comment\"># 等价于 .to(\"cuda\")</span></span><br><span class=\"line\">    z = x + y</span><br><span class=\"line\">    print(z)</span><br><span class=\"line\">    print(z.to(<span class=\"string\">\"cpu\"</span>, torch.double))       <span class=\"comment\"># to()还可以同时更改数据类型</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"线性回归\">线性回归<a href=\"#线性回归\" title=\"线性回归\"></a></h3><h4 id=\"1-线性回归的基本要素\">1. 线性回归的基本要素<a href=\"#1-线性回归的基本要素\" title=\"1. 线性回归的基本要素\"></a></h4><h5 id=\"模型\">模型<a href=\"#模型\" title=\"模型\"></a></h5><p>这里的例子是房屋价格预测，只考虑面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。线性回归假设输出与各个输入之间是线性关系：</p>\n<p><img src=\"https://s2.ax1x.com/2020/02/13/1OnSwd.png\" class=\"φcy\" alt=\"1\"></p>\n<h5 id=\"数据集\">数据集<a href=\"#数据集\" title=\"数据集\"></a></h5><p>我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为<strong>训练数据集</strong>（training data set）或训练集（training set），一栋<strong>房屋</strong>被称为一个<strong>样本</strong>（sample），其<strong>真实售出价格叫作标签</strong>（label），用来<strong>预测标签的两个因素叫作特征</strong>（feature）。特征用来表征样本的特点。</p>\n<h5 id=\"损失函数\">损失函数<a href=\"#损失函数\" title=\"损失函数\"></a></h5><p>在模型训练中，我们需要衡量价格预测值与真实值之间的<strong>误差</strong>。通常我们会选取一个非负数作为误差，且<strong>数值越小表示误差越小</strong>。一个常用的选择是<strong>平方函数</strong>。 它在评估索引为 i 的样本误差的表达式为</p>\n<p><img src=\"https://s2.ax1x.com/2020/02/13/1ON6Re.png\" class=\"φcy\" alt=\"2\"></p>\n<h5 id=\"优化函数---随机梯度下降\">优化函数 - 随机梯度下降<a href=\"#优化函数---随机梯度下降\" title=\"优化函数 - 随机梯度下降\"></a></h5><p>这里采用小批量随机梯度下降。</p>\n<h4 id=\"2-线性回归模型从零开始的实现\">2. 线性回归模型从零开始的实现<a href=\"#2-线性回归模型从零开始的实现\" title=\"2. 线性回归模型从零开始的实现\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># import packages and modules</span></span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> IPython <span class=\"keyword\">import</span> display</span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> random</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"生成数据集\">生成数据集<a href=\"#生成数据集\" title=\"生成数据集\"></a></h5><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs = <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"comment\"># set example number</span></span><br><span class=\"line\">num_examples = <span class=\"number\">1000</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># set true weight and bias in order to generate corresponded label</span></span><br><span class=\"line\">true_w = [<span class=\"number\">2</span>, <span class=\"number\">-3.4</span>]</span><br><span class=\"line\">true_b = <span class=\"number\">4.2</span></span><br><span class=\"line\"></span><br><span class=\"line\">features = torch.randn(num_examples, num_inputs,</span><br><span class=\"line\">                      dtype=torch.float32)\t<span class=\"comment\">#1000行，2列</span></span><br><span class=\"line\">labels = true_w[<span class=\"number\">0</span>] * features[:, <span class=\"number\">0</span>] + true_w[<span class=\"number\">1</span>] * features[:, <span class=\"number\">1</span>] + true_b</span><br><span class=\"line\"><span class=\"comment\">#为了使生成的数据集不完全符合上面的线性分布，加一个正态分布随机生成的偏差</span></span><br><span class=\"line\">labels += torch.tensor(np.random.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, size=labels.size()),</span><br><span class=\"line\">                       dtype=torch.float32)</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"读取数据集\">读取数据集<a href=\"#读取数据集\" title=\"读取数据集\"></a></h5><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">data_iter</span><span class=\"params\">(batch_size, features, labels)</span>:</span></span><br><span class=\"line\">    num_examples = len(features)</span><br><span class=\"line\">    indices = list(range(num_examples))</span><br><span class=\"line\">    random.shuffle(indices)  <span class=\"comment\"># random read 10 samples</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">0</span>, num_examples, batch_size):</span><br><span class=\"line\">        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) <span class=\"comment\"># the last time may be not enough for a whole batch</span></span><br><span class=\"line\">        <span class=\"keyword\">yield</span>  features.index_select(<span class=\"number\">0</span>, j), labels.index_select(<span class=\"number\">0</span>, j)</span><br></pre></td></tr></table></figure>\n\n<p>代码解释：</p>\n<ul><li><p>features是 1000*2 的张量，<code>num_examples = len(features)</code>是 1000，<code>indices = list(range(num_examples))</code>是0到999的一个list，<code>random.shuffle(indices)</code>把这1000个数打乱。这样后面取的时候就不会有序。</p></li><li><p>for循环次数取决于 batch_size 步长，每次从 indices[] 中取从 i 到 i + batch_size 序号的数，为了防止最后一次取超出了1000，所以加了<code>min(i + batch_size, num_examples)</code>。</p></li><li><p>torch.LongTensor 是CPU tensor的有符号64位int。更多数据类型参考<a href=\"https://www.jianshu.com/p/45a8579628c4\" target=\"_blank\">https://www.jianshu.com/p/45a8579628c4</a></p></li><li><p><code>index_select(0, j)</code>：从第0维挑选数据，从 j 中的位置挑选数据，j 中有是十个数字，也就是从 features 这10行挑选。</p></li></ul><p>可以看一下第一轮挑选出的数据：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size = <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> data_iter(batch_size, features, labels):</span><br><span class=\"line\">    print(X, <span class=\"string\">'\\n'</span>, y)</span><br><span class=\"line\">    <span class=\"keyword\">break</span></span><br></pre></td></tr></table></figure>\n\n<p>运行结果：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[-0.2732,  0.9252],</span><br><span class=\"line\">        [ 2.3337, -0.2072],</span><br><span class=\"line\">        [-0.4872, -0.4645],</span><br><span class=\"line\">        [-0.0217,  1.3339],</span><br><span class=\"line\">        [-0.7408,  0.0728],</span><br><span class=\"line\">        [ 1.7020, -0.5601],</span><br><span class=\"line\">        [ 0.0830,  0.7293],</span><br><span class=\"line\">        [-0.4804, -1.0335],</span><br><span class=\"line\">        [-1.3484, -1.5484],</span><br><span class=\"line\">        [ 0.1082, -0.2896]]) </span><br><span class=\"line\"> tensor([ 0.4939,  9.5747,  4.8044, -0.3721,  2.4826,  9.5104,  1.8949,  6.7509,</span><br><span class=\"line\">         6.7622,  5.4117])</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"初始化模型参数\">初始化模型参数<a href=\"#初始化模型参数\" title=\"初始化模型参数\"></a></h5><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#随机正态分布</span></span><br><span class=\"line\">w = torch.tensor(np.random.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, (num_inputs, <span class=\"number\">1</span>)), dtype=torch.float32)</span><br><span class=\"line\">b = torch.zeros(<span class=\"number\">1</span>, dtype=torch.float32)</span><br><span class=\"line\"></span><br><span class=\"line\">w.requires_grad_(requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">b.requires_grad_(requires_grad=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<p>代码解释：</p>\n<ul><li><p><code>numpy.random.normal(0, 0.01, (num_inputs, 1))</code>括号中第一个是loc，是概率分布的均值，第二个是scale，是概率分布的标准差，第三个是size。</p></li><li><p>requires_grad_()：给参数附加梯度，因为后面要用到参数的梯度。</p></li></ul><h5 id=\"定义模型\">定义模型<a href=\"#定义模型\" title=\"定义模型\"></a></h5><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">linreg</span><span class=\"params\">(X, w, b)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.mm(X, w) + b</span><br></pre></td></tr></table></figure>\n\n<p>代码解释：</p>\n<ul><li><code>torch.mm(a, b)</code> 是矩阵 a 和 b <strong>矩阵相乘</strong>，比如 a 的维度是 (1, 2)，b 的维度是 (2, 3)，返回的就是 (1, 3) 的矩阵</li></ul><h5 id=\"定义损失函数\">定义损失函数<a href=\"#定义损失函数\" title=\"定义损失函数\"></a></h5><p>使用的是均方误差损失函数：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">squared_loss</span><span class=\"params\">(y_hat, y)</span>:</span> </span><br><span class=\"line\">    <span class=\"keyword\">return</span> (y_hat - y.view(y_hat.size())) ** <span class=\"number\">2</span> / <span class=\"number\">2</span></span><br></pre></td></tr></table></figure>\n\n<h5 id=\"定义优化函数\">定义优化函数<a href=\"#定义优化函数\" title=\"定义优化函数\"></a></h5><p>使用的是小批量随机梯度下降</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">sgd</span><span class=\"params\">(params, lr, batch_size)</span>:</span> </span><br><span class=\"line\">    <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">        param.data -= lr * param.grad / batch_size <span class=\"comment\"># ues .data to operate param without gradient track对参数优化的动作不希望被附加</span></span><br></pre></td></tr></table></figure>\n\n<h5 id=\"训练\">训练<a href=\"#训练\" title=\"训练\"></a></h5><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># super parameters init</span></span><br><span class=\"line\">lr = <span class=\"number\">0.03</span>\t<span class=\"comment\">#学习率</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">5</span>\t<span class=\"comment\">#训练的周期</span></span><br><span class=\"line\"></span><br><span class=\"line\">net = linreg</span><br><span class=\"line\">loss = squared_loss</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># training</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(num_epochs):  <span class=\"comment\"># training repeats num_epochs times</span></span><br><span class=\"line\">    <span class=\"comment\"># in each epoch, all the samples in dataset will be used once</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># X is the feature and y is the label of a batch sample</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> data_iter(batch_size, features, labels):</span><br><span class=\"line\">        l = loss(net(X, w, b), y).sum()  </span><br><span class=\"line\">        <span class=\"comment\"># calculate the gradient of batch sample loss 反向传播求梯度</span></span><br><span class=\"line\">        l.backward()</span><br><span class=\"line\">        <span class=\"comment\"># using small batch random gradient descent to iter model parameters</span></span><br><span class=\"line\">        sgd([w, b], lr, batch_size)  </span><br><span class=\"line\">        <span class=\"comment\"># reset parameter gradient参数梯度要清零</span></span><br><span class=\"line\">        w.grad.data.zero_()</span><br><span class=\"line\">        b.grad.data.zero_()</span><br><span class=\"line\">    train_l = loss(net(features, w, b), labels)</span><br><span class=\"line\">    print(<span class=\"string\">'epoch %d, loss %f'</span> % (epoch + <span class=\"number\">1</span>, train_l.mean().item()))</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\">#输出：</span></span><br><span class=\"line\">epoch <span class=\"number\">1</span>, loss <span class=\"number\">0.032129</span></span><br><span class=\"line\">epoch <span class=\"number\">2</span>, loss <span class=\"number\">0.000109</span></span><br><span class=\"line\">epoch <span class=\"number\">3</span>, loss <span class=\"number\">0.000048</span></span><br><span class=\"line\">epoch <span class=\"number\">4</span>, loss <span class=\"number\">0.000048</span></span><br><span class=\"line\">epoch <span class=\"number\">5</span>, loss <span class=\"number\">0.000048</span></span><br></pre></td></tr></table></figure>\n\n<p>看一下训练得到的值和真实值的差距：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w, true_w, b, true_b</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#输出：</span></span><br><span class=\"line\">(tensor([[ <span class=\"number\">2.0005</span>],</span><br><span class=\"line\">         [<span class=\"number\">-3.3995</span>]], requires_grad=<span class=\"literal\">True</span>),</span><br><span class=\"line\"> [<span class=\"number\">2</span>, <span class=\"number\">-3.4</span>],</span><br><span class=\"line\"> tensor([<span class=\"number\">4.2005</span>], requires_grad=<span class=\"literal\">True</span>),</span><br><span class=\"line\"> <span class=\"number\">4.2</span>)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"3-线性回归模型使用pytorch的简洁实现\">3. 线性回归模型使用pytorch的简洁实现<a href=\"#3-线性回归模型使用pytorch的简洁实现\" title=\"3. 线性回归模型使用pytorch的简洁实现\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">1</span>)</span><br><span class=\"line\">torch.set_default_tensor_type(<span class=\"string\">'torch.FloatTensor'</span>)</span><br></pre></td></tr></table></figure>\n\n<p>在这里生成数据集跟从零开始的实现中是完全一样的。</p>\n<h5 id=\"读取数据集-1\">读取数据集<a href=\"#读取数据集-1\" title=\"读取数据集\"></a></h5><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.utils.data <span class=\"keyword\">as</span> Data</span><br><span class=\"line\"></span><br><span class=\"line\">batch_size = <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># combine featues and labels of dataset</span></span><br><span class=\"line\">dataset = Data.TensorDataset(features, labels)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># put dataset into DataLoader</span></span><br><span class=\"line\">data_iter = Data.DataLoader(</span><br><span class=\"line\">    dataset=dataset,            <span class=\"comment\"># torch TensorDataset format</span></span><br><span class=\"line\">    batch_size=batch_size,      <span class=\"comment\"># mini batch size</span></span><br><span class=\"line\">    shuffle=<span class=\"literal\">True</span>,               <span class=\"comment\"># whether shuffle the data or not</span></span><br><span class=\"line\">    num_workers=<span class=\"number\">2</span>,              <span class=\"comment\"># read data in multithreading</span></span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"定义模型-1\">定义模型<a href=\"#定义模型-1\" title=\"定义模型\"></a></h5><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LinearNet</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, n_feature)</span>:</span></span><br><span class=\"line\">        super(LinearNet, self).__init__()      <span class=\"comment\"># call father function to init </span></span><br><span class=\"line\">        self.linear = nn.Linear(n_feature, <span class=\"number\">1</span>)  <span class=\"comment\"># function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span></span><br><span class=\"line\">        y = self.linear(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> y</span><br><span class=\"line\">    </span><br><span class=\"line\">net = LinearNet(num_inputs)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#这里介绍初始化多层网络的三种方法</span></span><br><span class=\"line\"><span class=\"comment\"># ways to init a multilayer network</span></span><br><span class=\"line\"><span class=\"comment\"># method one</span></span><br><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">    nn.Linear(num_inputs, <span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># other layers can be added here</span></span><br><span class=\"line\">    )</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># method two</span></span><br><span class=\"line\">net = nn.Sequential()</span><br><span class=\"line\">net.add_module(<span class=\"string\">'linear'</span>, nn.Linear(num_inputs, <span class=\"number\">1</span>))</span><br><span class=\"line\"><span class=\"comment\"># net.add_module ......</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># method three</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> OrderedDict</span><br><span class=\"line\">net = nn.Sequential(OrderedDict([</span><br><span class=\"line\">          (<span class=\"string\">'linear'</span>, nn.Linear(num_inputs, <span class=\"number\">1</span>))</span><br><span class=\"line\">          <span class=\"comment\"># ......</span></span><br><span class=\"line\">        ]))</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"初始化模型参数-1\">初始化模型参数<a href=\"#初始化模型参数-1\" title=\"初始化模型参数\"></a></h5><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> init</span><br><span class=\"line\"></span><br><span class=\"line\">init.normal_(net[<span class=\"number\">0</span>].weight, mean=<span class=\"number\">0.0</span>, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">init.constant_(net[<span class=\"number\">0</span>].bias, val=<span class=\"number\">0.0</span>)  <span class=\"comment\"># or you can use `net[0].bias.data.fill_(0)` to modify it directly</span></span><br></pre></td></tr></table></figure>\n\n<h5 id=\"定义损失函数-1\">定义损失函数<a href=\"#定义损失函数-1\" title=\"定义损失函数\"></a></h5><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss = nn.MSELoss()    <span class=\"comment\"># nn built-in squared loss function</span></span><br><span class=\"line\">                       <span class=\"comment\"># function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')`</span></span><br></pre></td></tr></table></figure>\n\n<h5 id=\"定义优化函数-1\">定义优化函数<a href=\"#定义优化函数-1\" title=\"定义优化函数\"></a></h5><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"></span><br><span class=\"line\">optimizer = optim.SGD(net.parameters(), lr=<span class=\"number\">0.03</span>)   <span class=\"comment\"># built-in random gradient descent function</span></span><br><span class=\"line\">print(optimizer)  <span class=\"comment\"># function prototype: `torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)`</span></span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SGD (</span><br><span class=\"line\">Parameter Group 0</span><br><span class=\"line\">    dampening: 0</span><br><span class=\"line\">    lr: 0.03</span><br><span class=\"line\">    momentum: 0</span><br><span class=\"line\">    nesterov: False</span><br><span class=\"line\">    weight_decay: 0</span><br><span class=\"line\">)</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"训练-1\">训练<a href=\"#训练-1\" title=\"训练\"></a></h5><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs = <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, num_epochs + <span class=\"number\">1</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">        output = net(X)</span><br><span class=\"line\">        l = loss(output, y.view(<span class=\"number\">-1</span>, <span class=\"number\">1</span>))</span><br><span class=\"line\">        optimizer.zero_grad() <span class=\"comment\"># reset gradient, equal to net.zero_grad()</span></span><br><span class=\"line\">        l.backward()</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\">    print(<span class=\"string\">'epoch %d, loss: %f'</span> % (epoch, l.item()))</span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 1, loss: 0.000391</span><br><span class=\"line\">epoch 2, loss: 0.000126</span><br><span class=\"line\">epoch 3, loss: 0.000064</span><br></pre></td></tr></table></figure>\n\n<p>看结果：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># result comparision</span></span><br><span class=\"line\">dense = net[<span class=\"number\">0</span>]</span><br><span class=\"line\">print(true_w, dense.weight.data)</span><br><span class=\"line\">print(true_b, dense.bias.data)</span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[<span class=\"number\">2</span>, <span class=\"number\">-3.4</span>] tensor([[ <span class=\"number\">1.9996</span>, <span class=\"number\">-3.3994</span>]])</span><br><span class=\"line\"><span class=\"number\">4.2</span> tensor([<span class=\"number\">4.1994</span>])</span><br></pre></td></tr></table></figure>","prev":{"title":"pytorch笔记二：Softmax与分类模型","link":"2020/02/13/pytorch笔记二：Softmax与分类模型"},"next":{"title":"csp刷题笔记（第3-4题）","link":"2020/02/10/csp刷题笔记（第3-4题）"},"plink":"https://yuyuoo.github.io/2020/02/13/pytorch笔记一：线性回归/","toc":[{"id":"数据操作","title":"数据操作","index":"1"},{"id":"线性回归","title":"线性回归","index":"2"},{"id":"3-线性回归模型使用pytorch的简洁实现","title":"3. 线性回归模型使用pytorch的简洁实现","index":"3"}],"copyright":{"license":"Attribution-NonCommercial-NoDerivatives 4.0 International","published":"February 13, 2020","updated":"March 7, 2020","author":"YuYuoo"}}