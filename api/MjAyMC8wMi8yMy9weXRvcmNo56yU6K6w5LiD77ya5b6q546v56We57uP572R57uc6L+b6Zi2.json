{"title":"pytorch笔记七：循环神经网络进阶","date":"2020-02-23T04:09:58.000Z","date_formatted":{"ll":"Feb 23, 2020","L":"02/23/2020","MM-DD":"02-23"},"thumbnail":"https://s2.ax1x.com/2020/02/22/3Q8P4s.jpg","link":"2020/02/23/pytorch笔记七：循环神经网络进阶","comments":true,"tags":["pytorch"],"categories":["Notes"],"updated":"2021-08-10T02:04:09.474Z","content":"<h3 id=\"gru\">GRU<a href=\"#gru\" title=\"GRU\"></a></h3><p>RNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）\n门控循环神经网络：捕捉时间序列中时间步距离较大的依赖关系</p>\n<h5 id=\"rnn\">RNN:<a href=\"#rnn\" title=\"RNN:\"></a></h5><p><img src=\"https://i.loli.net/2021/08/02/IUPJYAiceKwubQR.png\" class=\"φcy\" alt=\"1.jpg\"></p>\n<p>$$\nH_{t} = ϕ(X_{t}W_{xh} + H_{t-1}W_{hh} + b_{h})\n$$</p>\n<h5 id=\"gru-1\">GRU:<a href=\"#gru-1\" title=\"GRU:\"></a></h5><p><img src=\"https://i.loli.net/2021/08/02/vGZHMVmaqd5oFxN.png\" class=\"φcy\" alt=\"2.jpg\"></p>\n<p>$$\n\\begin{align<em>}\nR_{t} = σ(X_tW_{xr} + H_{t−1}W_{hr} + b_r) \\\nZ_{t} = σ(X_tW_{xz} + H_{t−1}W_{hz} + b_z) \\\n\\widetilde{H}<em>t = tanh(X_tW</em>{xh} + (R_t ⊙H_{t−1})W_{hh} + b_h) \\\nH_t = Z_t⊙H_{t−1} + (1−Z_t)⊙\\widetilde{H}_t\n\\end{align</em>}\n$$</p>\n<ul><li>重置门有助于捕捉时间序列里短期的依赖关系。控制是否需要Ht-1，与Xt组合构成候选隐藏状态。重置门为0时，候选隐藏状态只包含Xt。</li><li>更新门有助于捕捉时间序列里长期的依赖关系。Zt为1时，Ht = Ht-1。</li></ul><h3 id=\"载入数据集\">载入数据集<a href=\"#载入数据集\" title=\"载入数据集\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\">os.listdir(<span class=\"string\">'/home/kesci/input'</span>)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn, optim</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">\"../input/\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> d2l_jay9460 <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\">device = torch.device(<span class=\"string\">'cuda'</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">'cpu'</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"初始化参数\">初始化参数<a href=\"#初始化参数\" title=\"初始化参数\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs, num_hiddens, num_outputs = vocab_size, <span class=\"number\">256</span>, vocab_size</span><br><span class=\"line\">print(<span class=\"string\">'will use'</span>, device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_params</span><span class=\"params\">()</span>:</span>  </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_one</span><span class=\"params\">(shape)</span>:</span></span><br><span class=\"line\">        ts = torch.tensor(np.random.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, size=shape), device=device, dtype=torch.float32) <span class=\"comment\">#正态分布</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.nn.Parameter(ts, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_three</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class=\"line\">                _one((num_hiddens, num_hiddens)),</span><br><span class=\"line\">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\">     </span><br><span class=\"line\">    W_xz, W_hz, b_z = _three()  <span class=\"comment\"># 更新门参数</span></span><br><span class=\"line\">    W_xr, W_hr, b_r = _three()  <span class=\"comment\"># 重置门参数</span></span><br><span class=\"line\">    W_xh, W_hh, b_h = _three()  <span class=\"comment\"># 候选隐藏状态参数</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 输出层参数</span></span><br><span class=\"line\">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class=\"line\">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.ParameterList([W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init_gru_state</span><span class=\"params\">(batch_size, num_hiddens, device)</span>:</span>   <span class=\"comment\">#隐藏状态初始化</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">will use cpu</span><br></pre></td></tr></table></figure>\n\n<ul><li>9+2个参数需要初始化。</li><li><code>_one()</code>函数用<code>torch.tensor(np.random.normal(0, 0.01, size=shape)</code>随机正态分布初始化，期望是0，方差是0.01。</li></ul><h3 id=\"gru模型\">GRU模型<a href=\"#gru模型\" title=\"GRU模型\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">gru</span><span class=\"params\">(inputs, state, params)</span>:</span></span><br><span class=\"line\">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class=\"line\">    H, = state</span><br><span class=\"line\">    outputs = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X <span class=\"keyword\">in</span> inputs:</span><br><span class=\"line\">        Z = torch.sigmoid(torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z)</span><br><span class=\"line\">        R = torch.sigmoid(torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r)</span><br><span class=\"line\">        H_tilda = torch.tanh(torch.matmul(X, W_xh) + R * torch.matmul(H, W_hh) + b_h)</span><br><span class=\"line\">        H = Z * H + (<span class=\"number\">1</span> - Z) * H_tilda</span><br><span class=\"line\">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class=\"line\">        outputs.append(Y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> outputs, (H,)</span><br></pre></td></tr></table></figure>\n\n<ul><li>模型的代码就是翻译上面的四行公式。</li></ul><h3 id=\"训练模型\">训练模型<a href=\"#训练模型\" title=\"训练模型\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class=\"number\">160</span>, <span class=\"number\">35</span>, <span class=\"number\">32</span>, <span class=\"number\">1e2</span>, <span class=\"number\">1e-2</span></span><br><span class=\"line\">pred_period, pred_len, prefixes = <span class=\"number\">40</span>, <span class=\"number\">50</span>, [<span class=\"string\">'分开'</span>, <span class=\"string\">'不分开'</span>]</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">d2l.train_and_predict_rnn(gru, get_params, init_gru_state, num_hiddens,</span><br><span class=\"line\">                          vocab_size, device, corpus_indices, idx_to_char,</span><br><span class=\"line\">                          char_to_idx, <span class=\"literal\">False</span>, num_epochs, num_steps, lr,</span><br><span class=\"line\">                          clipping_theta, batch_size, pred_period, pred_len,</span><br><span class=\"line\">                          prefixes)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 40, perplexity 149.271885, time 1.17 sec</span><br><span class=\"line\"> - 分开 我想我不不 我想你的让我 你想我的让我 你想我不想 我想你我想想想想想你想你的可爱人  坏我的让我</span><br><span class=\"line\"> - 不分开 我想你我不想 你不我 我想你的爱爱 我想你的让我 我想你我想想想想想想你的可爱人  坏我的让我 我</span><br><span class=\"line\">epoch 160, perplexity 1.427383, time 1.16 sec</span><br><span class=\"line\"> - 分开 我已带口 你已已是不起 让你知没面对我 甩散球我满腔的怒火 我想揍你已经很久 别想躲 说你眼睛看着</span><br><span class=\"line\"> - 不分开 整过 是你开的玩笑 想通 却又再考倒我 说散 你想很久了吧? 败给你的黑色幽默 说散 你想很久了吧</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"简洁实现\">简洁实现<a href=\"#简洁实现\" title=\"简洁实现\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_hiddens=<span class=\"number\">256</span></span><br><span class=\"line\">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class=\"number\">160</span>, <span class=\"number\">35</span>, <span class=\"number\">32</span>, <span class=\"number\">1e2</span>, <span class=\"number\">1e-2</span></span><br><span class=\"line\">pred_period, pred_len, prefixes = <span class=\"number\">40</span>, <span class=\"number\">50</span>, [<span class=\"string\">'分开'</span>, <span class=\"string\">'不分开'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">lr = <span class=\"number\">1e-2</span> <span class=\"comment\"># 注意调整学习率</span></span><br><span class=\"line\">gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class=\"line\">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class=\"line\">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class=\"line\">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class=\"line\">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class=\"line\">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure>\n<ul><li>不用自己初始化参数和写模型代码。只需调用<code>nn.GRU()</code>生成gru_layer。</li></ul><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 40, perplexity 1.016101, time 0.89 sec</span><br><span class=\"line\"> - 分开始想像 爸和妈当年的模样 说著一口吴侬软语的姑娘缓缓走过外滩 消失的 旧时光 一九四三 回头看 的片</span><br><span class=\"line\"> - 不分开暴风圈来不及逃 我不能再想 我不能再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我已无处</span><br><span class=\"line\">epoch 80, perplexity 1.010881, time 0.96 sec</span><br><span class=\"line\"> - 分开都会值得去做 我想大声宣布 对你依依不舍 连隔壁邻居都猜到我现在的感受 河边的风 在吹着头发飘动 牵</span><br><span class=\"line\"> - 不分开暴风圈来不及逃 我不能再想 我不能再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我已无处</span><br><span class=\"line\">epoch 120, perplexity 1.011403, time 0.95 sec</span><br><span class=\"line\"> - 分开的我爱你看棒球 想这样没担忧 唱着歌 一直走 我想就这样牵着你的手不放开 爱可不可以简简单单没有伤害</span><br><span class=\"line\"> - 不分开暴风圈来不及逃 我不能再想 我不能再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我已无处</span><br><span class=\"line\">epoch 160, perplexity 1.058085, time 0.88 sec</span><br><span class=\"line\"> - 分开始打呼 管到当初爱你的时空 停格内容不忠 所有回忆对着我进攻   简单爱情来的太快就像龙卷风 离不开</span><br><span class=\"line\"> - 不分开始打呼 管家是一只是我怕眼泪撑不住 不懂 你给我抬起头 有话去对医药箱说 别怪我 别怪我 说你怎么面</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"lstm\">LSTM<a href=\"#lstm\" title=\"LSTM\"></a></h3><p><strong>长短期记忆long short-term memory</strong> :\n遗忘门:控制上一时间步的记忆细胞。如果遗忘门是0，就相当于忘记记忆细胞Ct-1，Ct-1就不会被加到Ct上。\n输入门:控制当前时间步的输入。控制候选记忆细胞有多少被加到Ct上。如果输入门是1，候选记忆细胞就全部输入到Ct中。\n输出门:控制从记忆细胞输入多少到隐藏状态。\n记忆细胞：⼀种特殊的隐藏状态的信息的流动。</p>\n<img src=\"https://i.loli.net/2021/08/03/OwBfdpcrnt149RH.png\" alt=\"3.jpg\" style=\"zoom: 80%;\">\n$$\n\\begin{align*}\nI_t = σ(X_tW_{xi} + H_{t−1}W_{hi} + b_i) \\\\\nF_t = σ(X_tW_{xf} + H_{t−1}W_{hf} + b_f)\\\\\nO_t = σ(X_tW_{xo} + H_{t−1}W_{ho} + b_o)\\\\\n\\widetilde{C}_t = tanh(X_tW_{xc} + H_{t−1}W_{hc} + b_c)\\\\\nC_t = F_t ⊙C_{t−1} + I_t ⊙\\widetilde{C}_t\\\\\nH_t = O_t⊙tanh(C_t)\n\\end{align*}\n$$\n\n<ul><li>遗忘门、输入门、输出门都是由输入和隐藏状态组成的。</li></ul><h3 id=\"初始化参数-1\">初始化参数<a href=\"#初始化参数-1\" title=\"初始化参数\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs, num_hiddens, num_outputs = vocab_size, <span class=\"number\">256</span>, vocab_size</span><br><span class=\"line\">print(<span class=\"string\">'will use'</span>, device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_params</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_one</span><span class=\"params\">(shape)</span>:</span></span><br><span class=\"line\">        ts = torch.tensor(np.random.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, size=shape), device=device, dtype=torch.float32)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.nn.Parameter(ts, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_three</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class=\"line\">                _one((num_hiddens, num_hiddens)),</span><br><span class=\"line\">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class=\"literal\">True</span>))</span><br><span class=\"line\">    </span><br><span class=\"line\">    W_xi, W_hi, b_i = _three()  <span class=\"comment\"># 输入门参数</span></span><br><span class=\"line\">    W_xf, W_hf, b_f = _three()  <span class=\"comment\"># 遗忘门参数</span></span><br><span class=\"line\">    W_xo, W_ho, b_o = _three()  <span class=\"comment\"># 输出门参数</span></span><br><span class=\"line\">    W_xc, W_hc, b_c = _three()  <span class=\"comment\"># 候选记忆细胞参数</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 输出层参数</span></span><br><span class=\"line\">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class=\"line\">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init_lstm_state</span><span class=\"params\">(batch_size, num_hiddens, device)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (torch.zeros((batch_size, num_hiddens), device=device), </span><br><span class=\"line\">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">will use cpu</span><br></pre></td></tr></table></figure>\n\n<ul><li>初始化12个参数。</li></ul><h3 id=\"lstm模型\">LSTM模型<a href=\"#lstm模型\" title=\"LSTM模型\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">lstm</span><span class=\"params\">(inputs, state, params)</span>:</span></span><br><span class=\"line\">    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params</span><br><span class=\"line\">    (H, C) = state</span><br><span class=\"line\">    outputs = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X <span class=\"keyword\">in</span> inputs:</span><br><span class=\"line\">        I = torch.sigmoid(torch.matmul(X, W_xi) + torch.matmul(H, W_hi) + b_i)</span><br><span class=\"line\">        F = torch.sigmoid(torch.matmul(X, W_xf) + torch.matmul(H, W_hf) + b_f)</span><br><span class=\"line\">        O = torch.sigmoid(torch.matmul(X, W_xo) + torch.matmul(H, W_ho) + b_o)</span><br><span class=\"line\">        C_tilda = torch.tanh(torch.matmul(X, W_xc) + torch.matmul(H, W_hc) + b_c)</span><br><span class=\"line\">        C = F * C + I * C_tilda</span><br><span class=\"line\">        H = O * C.tanh()</span><br><span class=\"line\">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class=\"line\">        outputs.append(Y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> outputs, (H, C)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"训练模型-1\">训练模型<a href=\"#训练模型-1\" title=\"训练模型\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class=\"number\">160</span>, <span class=\"number\">35</span>, <span class=\"number\">32</span>, <span class=\"number\">1e2</span>, <span class=\"number\">1e-2</span></span><br><span class=\"line\">pred_period, pred_len, prefixes = <span class=\"number\">40</span>, <span class=\"number\">50</span>, [<span class=\"string\">'分开'</span>, <span class=\"string\">'不分开'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.train_and_predict_rnn(lstm, get_params, init_lstm_state, num_hiddens,</span><br><span class=\"line\">                          vocab_size, device, corpus_indices, idx_to_char,</span><br><span class=\"line\">                          char_to_idx, <span class=\"literal\">False</span>, num_epochs, num_steps, lr,</span><br><span class=\"line\">                          clipping_theta, batch_size, pred_period, pred_len,</span><br><span class=\"line\">                          prefixes)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 40, perplexity 211.457328, time 1.51 sec</span><br><span class=\"line\"> - 分开 我不的我 我不的我 我不不 我不的我 我不不 我不的我 我不不 我不的我 我不不 我不的我 我不不</span><br><span class=\"line\"> - 不分开 我不不 我不的我 我不不 我不的我 我不不 我不的我 我不不 我不的我 我不不 我不的我 我不不 </span><br><span class=\"line\">epoch 80, perplexity 68.458662, time 1.50 sec</span><br><span class=\"line\"> - 分开 我想你这你 我不要这你 我不要这你 我不要这你 我不要这你 我不要这你 我不要这你 我不要这你 我</span><br><span class=\"line\"> - 不分开 我想你你的你 我想要你 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我</span><br><span class=\"line\">epoch 120, perplexity 15.034657, time 1.49 sec</span><br><span class=\"line\"> - 分开 我想你你的你笑 不知不觉 你你了一我不我 别发抖 快给我抬起起着你 别发抖 快给我抬起头 有你去对</span><br><span class=\"line\"> - 不分开 我想你你 我不要再想我 不知不觉 你你了离不我 不知不觉 你跟了离不我 不知不觉 我该了这节活 后</span><br><span class=\"line\">epoch 160, perplexity 3.897414, time 1.49 sec</span><br><span class=\"line\"> - 分开 我想带你里嵩山 学少林跟了了刚 我想就你了嵩着 我想去这生嵩 不天到双截棍 哼哼哈兮 快使用双截棍</span><br><span class=\"line\"> - 不分开 我 我你你的微笑 像通  又又我 我想就这样牵着你的手不放  穿过来回单单 我 想和你样堡堡 我想</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"简洁实现-1\">简洁实现<a href=\"#简洁实现-1\" title=\"简洁实现\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_hiddens=<span class=\"number\">256</span></span><br><span class=\"line\">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class=\"number\">160</span>, <span class=\"number\">35</span>, <span class=\"number\">32</span>, <span class=\"number\">1e2</span>, <span class=\"number\">1e-2</span></span><br><span class=\"line\">pred_period, pred_len, prefixes = <span class=\"number\">40</span>, <span class=\"number\">50</span>, [<span class=\"string\">'分开'</span>, <span class=\"string\">'不分开'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">lr = <span class=\"number\">1e-2</span> <span class=\"comment\"># 注意调整学习率</span></span><br><span class=\"line\">lstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class=\"line\">model = d2l.RNNModel(lstm_layer, vocab_size)</span><br><span class=\"line\">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class=\"line\">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class=\"line\">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class=\"line\">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 40, perplexity 1.019881, time 1.04 sec</span><br><span class=\"line\"> - 分开始打呼 管家是一只会说法语举止优雅的猪 吸血前会念约翰福音做为弥补 拥有一双蓝色眼睛的凯萨琳公主 专</span><br><span class=\"line\"> - 不分开的玩笑 想通 却又再考倒我 说散 你想很久了吧? 败给你的黑色幽默 不想太多 我想一定是我听错弄错搞</span><br><span class=\"line\">epoch 80, perplexity 1.013078, time 1.01 sec</span><br><span class=\"line\"> - 分开的话像语言暴力 我已无能为力再提起 决定中断熟悉 然后在这里 不限日期 然后将过去 慢慢温习 让我爱</span><br><span class=\"line\"> - 不分开的玩笑 想通 却又再考倒我 说散 你想很久了吧? 败给你的黑色幽默 说散 你想很久了吧? 我的认真败</span><br><span class=\"line\">epoch 120, perplexity 1.010264, time 1.01 sec</span><br><span class=\"line\"> - 分开 我们儿子她人在江南等我 泪不休 语沉默 一壶好酒 再来一碗热粥 配上几斤的牛肉 我说店小二 三两银</span><br><span class=\"line\"> - 不分开 我有你看棒球 想这样没担忧 唱着歌 一直走 我想就这样牵着你的手不放开 爱可不可以简简单单没有伤害</span><br><span class=\"line\">epoch 160, perplexity 1.008950, time 1.02 sec</span><br><span class=\"line\"> - 分开 我才  原来我只想要你 陪我去吃汉堡  说穿了其实我的愿望就怎么小 就怎么每天祈祷我的心跳你知道 </span><br><span class=\"line\"> - 不分开 我才你看 我想要再这样打我妈妈 我说的话 你甘会听 不要再这样打我妈妈 难道你手不会痛吗 其实我回</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"深度循环神经网络\">深度循环神经网络<a href=\"#深度循环神经网络\" title=\"深度循环神经网络\"></a></h2><p><img src=\"https://i.loli.net/2021/08/03/TKNv9ESyxiCJZmr.png\" class=\"φcy\" alt=\"4.jpg\"></p>\n<p>$$\n\\begin{align<em>}\n\\boldsymbol{H}<em>t^{(1)} = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}</em>{xh}^{(1)} + \\boldsymbol{H}<em>{t-1}^{(1)} \\boldsymbol{W}</em>{hh}^{(1)} + \\boldsymbol{b}<em>h^{(1)})\\\n\\boldsymbol{H}_t^{(\\ell)} = \\phi(\\boldsymbol{H}_t^{(\\ell-1)} \\boldsymbol{W}</em>{xh}^{(\\ell)} + \\boldsymbol{H}<em>{t-1}^{(\\ell)} \\boldsymbol{W}</em>{hh}^{(\\ell)} + \\boldsymbol{b}<em>h^{(\\ell)})\\\n\\boldsymbol{O}_t = \\boldsymbol{H}_t^{(L)} \\boldsymbol{W}</em>{hq} + \\boldsymbol{b}_q\n\\end{align</em>}\n$$</p>\n<p>GRU只有这张图的前两列，隐藏层H2的输入是H1和X2，以此类推。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_hiddens=<span class=\"number\">256</span></span><br><span class=\"line\">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class=\"number\">160</span>, <span class=\"number\">35</span>, <span class=\"number\">32</span>, <span class=\"number\">1e2</span>, <span class=\"number\">1e-2</span></span><br><span class=\"line\">pred_period, pred_len, prefixes = <span class=\"number\">40</span>, <span class=\"number\">50</span>, [<span class=\"string\">'分开'</span>, <span class=\"string\">'不分开'</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">lr = <span class=\"number\">1e-2</span> <span class=\"comment\"># 注意调整学习率</span></span><br><span class=\"line\"></span><br><span class=\"line\">gru_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens,num_layers=<span class=\"number\">2</span>)</span><br><span class=\"line\">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class=\"line\">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class=\"line\">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class=\"line\">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class=\"line\">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 40, perplexity 12.840496, time 1.52 sec</span><br><span class=\"line\"> - 分开我 想你的话我在想再你的让我女疼 我想你 我有要有 想你你 想你的让我女沉 我想你你 想你的让我女沉</span><br><span class=\"line\"> - 不分开的经爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我</span><br><span class=\"line\">epoch 80, perplexity 1.247634, time 1.52 sec</span><br><span class=\"line\"> - 分开有一条热昏头的响尾蛇 无力的躺在干枯的河 在等待雨季来临变沼泽 灰狼啃食著水鹿的骨头 秃鹰盘旋死盯着</span><br><span class=\"line\"> - 不分开的会手 穿梭放受 一朵一朵因你而香 试图让夕阳飞翔 带领你我环绕大自然 迎著风 开始共渡每一天 手牵</span><br><span class=\"line\">epoch 120, perplexity 1.021974, time 1.56 sec</span><br><span class=\"line\"> - 分开我妈妈 我有多重要 我后悔没让你知道 安静的听你撒娇 看你睡著一直到老 就是开不了口让她知道 就是那</span><br><span class=\"line\"> - 不分开的会堡  想要将我不投 又不会掩护我 选你这种队友 瞎透了我 说你说 分数怎么停留 一直在停留 谁让</span><br><span class=\"line\">epoch 160, perplexity 1.016324, time 1.59 sec</span><br><span class=\"line\"> - 分开在没有一个人身留  旧时光 一九四三 在回忆 的路上 时间变好慢 老街坊 小弄堂 是属于那年代白墙黑</span><br><span class=\"line\"> - 不分开的我有 有样的要再这样打我妈妈 难道你手不会痛吗 不要再这样打我妈妈 难道你手不会痛吗 不要再这样打</span><br></pre></td></tr></table></figure>\n\n<ul><li>设置<code>nn.LSTM()</code>的参数<code>num_layers=2</code>，也就是两个隐藏层，不写默认隐藏层是一层。</li><li>不是越深越好，越深模型越复杂，对数据集的要求越高，内容更抽象。</li></ul><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gru_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens,num_layers=<span class=\"number\">6</span>)</span><br><span class=\"line\">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class=\"line\">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class=\"line\">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class=\"line\">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class=\"line\">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 40, perplexity 276.815235, time 8.50 sec</span><br><span class=\"line\"> - 分开                                                  </span><br><span class=\"line\"> - 不分开                                                  </span><br><span class=\"line\">epoch 80, perplexity 276.278550, time 8.51 sec</span><br><span class=\"line\"> - 分开                                                  </span><br><span class=\"line\"> - 不分开                                                  </span><br><span class=\"line\">epoch 120, perplexity 276.146710, time 8.53 sec</span><br><span class=\"line\"> - 分开                                                  </span><br><span class=\"line\"> - 不分开                                                  </span><br><span class=\"line\">epoch 160, perplexity 275.739864, time 9.04 sec</span><br><span class=\"line\"> - 分开                                                  </span><br><span class=\"line\"> - 不分开</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"双向循环神经网络\">双向循环神经网络<a href=\"#双向循环神经网络\" title=\"双向循环神经网络\"></a></h2><p><img src=\"https://i.loli.net/2021/08/03/yAoQSCODZsRhFMK.png\" class=\"φcy\" alt=\"5.jpg\"></p>\n<p>$$\n\\begin{align<em>}\n\\overrightarrow{\\boldsymbol{H}}<em>t &amp;= \\phi(\\boldsymbol{X}_t \\boldsymbol{W}</em>{xh}^{(f)} + \\overrightarrow{\\boldsymbol{H}}<em>{t-1} \\boldsymbol{W}</em>{hh}^{(f)} + \\boldsymbol{b}<em>h^{(f)})\\\n\\overleftarrow{\\boldsymbol{H}}_t &amp;= \\phi(\\boldsymbol{X}_t \\boldsymbol{W}</em>{xh}^{(b)} + \\overleftarrow{\\boldsymbol{H}}<em>{t+1} \\boldsymbol{W}</em>{hh}^{(b)} + \\boldsymbol{b}<em>h^{(b)})\\\n\\boldsymbol{H}_t=(\\overrightarrow{\\boldsymbol{H}}</em>{t}, \\overleftarrow{\\boldsymbol{H}}<em>t)\\\n\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}</em>{hq} + \\boldsymbol{b}_q\n\\end{align</em>}\n$$</p>\n<ul><li>有从前X1往Xt的方向，也有Xt往X1的方向。</li><li>好处：原来只考虑了每个字前面的字对它的影响，现在前后字的影响都考虑。</li><li>在<code>nn.GRU()</code>中加入<code>bidirectional=True</code>参数，就是双向。这个参数默认是false。</li></ul><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 40, perplexity 1.001741, time 0.91 sec</span><br><span class=\"line\"> - 分开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开</span><br><span class=\"line\"> - 不分开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开</span><br><span class=\"line\">epoch 80, perplexity 1.000520, time 0.91 sec</span><br><span class=\"line\"> - 分开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开</span><br><span class=\"line\"> - 不分开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开</span><br><span class=\"line\">epoch 120, perplexity 1.000255, time 0.99 sec</span><br><span class=\"line\"> - 分开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开</span><br><span class=\"line\"> - 不分开球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我</span><br><span class=\"line\">epoch 160, perplexity 1.000151, time 0.92 sec</span><br><span class=\"line\"> - 分开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开</span><br><span class=\"line\"> - 不分开球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我球我</span><br></pre></td></tr></table></figure>\n\n<ul><li>结果显示双向循环NN的方法在这里是不合适的。因为这里是生成式语言模型，双向也会带来更多的参数，所以效果未必会好。</li></ul>","prev":{"title":"不定期更新的个人词典","link":"2020/03/07/不定期更新的个人词典"},"next":{"title":"pytorch笔记六：循环神经网络基础","link":"2020/02/17/pytorch笔记六：循环神经网络基础"},"plink":"https://yuyuoo.github.io/2020/02/23/pytorch笔记七：循环神经网络进阶/","toc":[{"id":"gru","title":"GRU","index":"1"},{"id":"载入数据集","title":"载入数据集","index":"2"},{"id":"初始化参数","title":"初始化参数","index":"3"},{"id":"gru模型","title":"GRU模型","index":"4"},{"id":"训练模型","title":"训练模型","index":"5"},{"id":"简洁实现","title":"简洁实现","index":"6"},{"id":"lstm","title":"LSTM","index":"7"},{"id":"初始化参数-1","title":"初始化参数","index":"8"},{"id":"lstm模型","title":"LSTM模型","index":"9"},{"id":"训练模型-1","title":"训练模型","index":"10"},{"id":"简洁实现-1","title":"简洁实现","index":"11"}],"copyright":{"license":"Attribution-NonCommercial-NoDerivatives 4.0 International","published":"February 23, 2020","updated":"August 10, 2021","author":"YuYuoo"}}