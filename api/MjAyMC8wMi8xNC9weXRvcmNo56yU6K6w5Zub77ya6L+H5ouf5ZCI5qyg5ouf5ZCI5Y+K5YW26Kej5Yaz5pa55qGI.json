{"title":"pytorch笔记四：过拟合欠拟合及其解决方案","date":"2020-02-14T07:40:45.000Z","date_formatted":{"ll":"Feb 14, 2020","L":"02/14/2020","MM-DD":"02-14"},"thumbnail":"https://i.loli.net/2020/02/14/1LWrEJvD8koPMnB.jpg","link":"2020/02/14/pytorch笔记四：过拟合欠拟合及其解决方案","comments":true,"tags":["pytorch"],"categories":["Notes"],"updated":"2020-03-07T06:40:51.164Z","content":"<h3 id=\"1-模型选择、过拟合和欠拟合\">1. 模型选择、过拟合和欠拟合<a href=\"#1-模型选择、过拟合和欠拟合\" title=\"1. 模型选择、过拟合和欠拟合\"></a></h3><h4 id=\"训练误差和泛化误差\">训练误差和泛化误差<a href=\"#训练误差和泛化误差\" title=\"训练误差和泛化误差\"></a></h4><p>在解释上述现象之前，我们需要区分训练误差（training error）和泛化误差（generalization error）。通俗来讲，前者指模型<strong>在训练数据集上表现出的误差</strong>，后者指模型<strong>在任意一个测试数据样本上表现出的误差的期望</strong>，并常常通过测试数据集上的误差来近似。计算训练误差和泛化误差可以使用之前介绍过的损失函数，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数。</p>\n<p>机器学习模型应<strong>关注降低泛化误差</strong>。</p>\n<h4 id=\"模型选择\">模型选择<a href=\"#模型选择\" title=\"模型选择\"></a></h4><h5 id=\"验证数据集\">验证数据集<a href=\"#验证数据集\" title=\"验证数据集\"></a></h5><p>从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以<strong>预留一部分在训练数据集和测试数据集以外的数据来进行模型选择</strong>。这部分数据被称为验证数据集，简称<strong>验证集</strong>（validation set）。例如，我们可以<strong>从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集</strong>。</p>\n<h5 id=\"k折交叉验证\">K折交叉验证<a href=\"#k折交叉验证\" title=\"K折交叉验证\"></a></h5><p>由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是K折交叉验证（K-fold cross-validation）。在K折交叉验证中，我们<strong>把原始训练数据集分割成K个不重合的子数据集</strong>，然后我们<strong>做K次模型训练和验证</strong>。每一次，我们使用一个子数据集验证模型，并<strong>使用其他K-1个子数据集来训练模型</strong>。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们<strong>对这K次训练误差和验证误差分别求平均</strong>。</p>\n<h4 id=\"过拟合和欠拟合\">过拟合和欠拟合<a href=\"#过拟合和欠拟合\" title=\"过拟合和欠拟合\"></a></h4><ul><li>一类是模型<strong>无法得到较低的训练误差</strong>，我们将这一现象称作欠拟合（underfitting）；</li><li>另一类是<strong>模型的训练误差远小于它在测试数据集上的误差</strong>，我们称该现象为过拟合（overfitting）。 在实践中，我们要尽可能同时应对欠拟合和过拟合。虽然有很多因素可能导致这两种拟合问题，在这里我们重点讨论两个因素：模型复杂度和训练数据集大小。</li></ul><h5 id=\"模型复杂度\">模型复杂度<a href=\"#模型复杂度\" title=\"模型复杂度\"></a></h5><p><img src=\"https://i.loli.net/2020/02/14/dKGzk83FgXOxpMW.png\" class=\"φcy\" alt=\"1\"></p>\n<h5 id=\"训练数据集大小\">训练数据集大小<a href=\"#训练数据集大小\" title=\"训练数据集大小\"></a></h5><p>影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果<strong>训练数据集中样本数过少</strong>，特别是比模型参数数量（按元素计）更少时，<strong>过拟合更容易发生</strong>。此外，<strong>泛化误差不会随训练数据集里样本数量增加而增大</strong>。因此，在计算资源允许的范围之内，我们通常<strong>希望训练数据集大一些</strong>，特别是在模型复杂度较高时，例如层数较多的深度学习模型。</p>\n<h3 id=\"2-多项式函数拟合实验\">2. 多项式函数拟合实验<a href=\"#2-多项式函数拟合实验\" title=\"2. 多项式函数拟合实验\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">\"/home/kesci/input\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> d2lzh1981 <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"初始化模型参数\">初始化模型参数<a href=\"#初始化模型参数\" title=\"初始化模型参数\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n_train, n_test, true_w, true_b = <span class=\"number\">100</span>, <span class=\"number\">100</span>, [<span class=\"number\">1.2</span>, <span class=\"number\">-3.4</span>, <span class=\"number\">5.6</span>], <span class=\"number\">5</span></span><br><span class=\"line\">features = torch.randn((n_train + n_test, <span class=\"number\">1</span>))</span><br><span class=\"line\">poly_features = torch.cat((features, torch.pow(features, <span class=\"number\">2</span>), torch.pow(features, <span class=\"number\">3</span>)), <span class=\"number\">1</span>) </span><br><span class=\"line\">labels = (true_w[<span class=\"number\">0</span>] * poly_features[:, <span class=\"number\">0</span>] + true_w[<span class=\"number\">1</span>] * poly_features[:, <span class=\"number\">1</span>]</span><br><span class=\"line\">          + true_w[<span class=\"number\">2</span>] * poly_features[:, <span class=\"number\">2</span>] + true_b)</span><br><span class=\"line\">labels += torch.tensor(np.random.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, size=labels.size()), dtype=torch.float)</span><br></pre></td></tr></table></figure>\n\n<ul><li><code>poly_features</code>是聚合的特征，将<code>features</code>看做 X ,<code>poly_features</code>就是 X , X 的平方，X 的三次方。</li><li><code>torch.cat</code>把这三个张量拼在一起，参数1表示按行拼（横着拼）。</li></ul><h4 id=\"定义、训练和测试模型\">定义、训练和测试模型<a href=\"#定义、训练和测试模型\" title=\"定义、训练和测试模型\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 观察训练误差和泛化误差间的关系</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">semilogy</span><span class=\"params\">(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">             legend=None, figsize=<span class=\"params\">(<span class=\"number\">3.5</span>, <span class=\"number\">2.5</span>)</span>)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># d2l.set_figsize(figsize)</span></span><br><span class=\"line\">    d2l.plt.xlabel(x_label)</span><br><span class=\"line\">    d2l.plt.ylabel(y_label)</span><br><span class=\"line\">    d2l.plt.semilogy(x_vals, y_vals)</span><br><span class=\"line\">    <span class=\"keyword\">if</span> x2_vals <span class=\"keyword\">and</span> y2_vals:</span><br><span class=\"line\">        d2l.plt.semilogy(x2_vals, y2_vals, linestyle=<span class=\"string\">':'</span>)</span><br><span class=\"line\">        d2l.plt.legend(legend)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, loss = <span class=\"number\">100</span>, torch.nn.MSELoss()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fit_and_plot</span><span class=\"params\">(train_features, test_features, train_labels, test_labels)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 初始化网络模型</span></span><br><span class=\"line\">    net = torch.nn.Linear(train_features.shape[<span class=\"number\">-1</span>], <span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 通过Linear文档可知，pytorch已经将参数初始化了，所以我们这里就不手动初始化了</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\"># 设置批量大小</span></span><br><span class=\"line\">    batch_size = min(<span class=\"number\">10</span>, train_labels.shape[<span class=\"number\">0</span>])    </span><br><span class=\"line\">    dataset = torch.utils.data.TensorDataset(train_features, train_labels)      <span class=\"comment\"># 设置数据集</span></span><br><span class=\"line\">    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class=\"literal\">True</span>) <span class=\"comment\"># 设置获取数据方式</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    optimizer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.01</span>)                      <span class=\"comment\"># 设置优化函数，使用的是随机梯度下降优化</span></span><br><span class=\"line\">    train_ls, test_ls = [], []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:                                                 <span class=\"comment\"># 取一个批量的数据</span></span><br><span class=\"line\">            l = loss(net(X), y.view(<span class=\"number\">-1</span>, <span class=\"number\">1</span>))                                     <span class=\"comment\"># 输入到网络中计算输出，并和标签比较求得损失函数</span></span><br><span class=\"line\">            optimizer.zero_grad()                                               <span class=\"comment\"># 梯度清零，防止梯度累加干扰优化</span></span><br><span class=\"line\">            l.backward()                                                        <span class=\"comment\"># 求梯度</span></span><br><span class=\"line\">            optimizer.step()                                                    <span class=\"comment\"># 迭代优化函数，进行参数优化</span></span><br><span class=\"line\">        train_labels = train_labels.view(<span class=\"number\">-1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">        test_labels = test_labels.view(<span class=\"number\">-1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">        train_ls.append(loss(net(train_features), train_labels).item())         <span class=\"comment\"># 将训练损失保存到train_ls中</span></span><br><span class=\"line\">        test_ls.append(loss(net(test_features), test_labels).item())            <span class=\"comment\"># 将测试损失保存到test_ls中</span></span><br><span class=\"line\">    print(<span class=\"string\">'final epoch: train loss'</span>, train_ls[<span class=\"number\">-1</span>], <span class=\"string\">'test loss'</span>, test_ls[<span class=\"number\">-1</span>])    </span><br><span class=\"line\">    semilogy(range(<span class=\"number\">1</span>, num_epochs + <span class=\"number\">1</span>), train_ls, <span class=\"string\">'epochs'</span>, <span class=\"string\">'loss'</span>,</span><br><span class=\"line\">             range(<span class=\"number\">1</span>, num_epochs + <span class=\"number\">1</span>), test_ls, [<span class=\"string\">'train'</span>, <span class=\"string\">'test'</span>])</span><br><span class=\"line\">    print(<span class=\"string\">'weight:'</span>, net.weight.data,</span><br><span class=\"line\">          <span class=\"string\">'\\nbias:'</span>, net.bias.data)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"三阶多项式函数拟合（正常）\">三阶多项式函数拟合（正常）<a href=\"#三阶多项式函数拟合（正常）\" title=\"三阶多项式函数拟合（正常）\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">final epoch: train loss 8.44636233523488e-05 test loss 0.00010286522592650726</span><br><span class=\"line\">weight: tensor([[ 1.1989, -3.4000,  5.6007]]) </span><br><span class=\"line\">bias: tensor([4.9991])</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://s2.ax1x.com/2020/02/15/1vqQVe.png\" class=\"φcy\" alt=\"2\"></p>\n<h4 id=\"线性函数拟合（欠拟合）\">线性函数拟合（欠拟合）<a href=\"#线性函数拟合（欠拟合）\" title=\"线性函数拟合（欠拟合）\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train], labels[n_train:])</span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">final epoch: train loss 144.07913208007812 test loss 66.60337829589844</span><br><span class=\"line\">weight: tensor([[16.6699]]) </span><br><span class=\"line\">bias: tensor([1.2409])</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://s2.ax1x.com/2020/02/15/1vqG8I.png\" class=\"φcy\" alt=\"3\"></p>\n<h4 id=\"训练样本不足（过拟合）\">训练样本不足（过拟合）<a href=\"#训练样本不足（过拟合）\" title=\"训练样本不足（过拟合）\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fit_and_plot(poly_features[<span class=\"number\">0</span>:<span class=\"number\">2</span>, :], poly_features[n_train:, :], labels[<span class=\"number\">0</span>:<span class=\"number\">2</span>], labels[n_train:])</span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">final epoch: train loss 0.3434029221534729 test loss 171.13600158691406</span><br><span class=\"line\">weight: tensor([[ 0.7433,  0.4778, -0.0182]]) </span><br><span class=\"line\">bias: tensor([4.3294])</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://s2.ax1x.com/2020/02/15/1vq0aQ.png\" class=\"φcy\" alt=\"4\"></p>\n<h3 id=\"3-权重衰减\">3. 权重衰减<a href=\"#3-权重衰减\" title=\"3. 权重衰减\"></a></h3><p>权重衰减等价于 L2 范数正则化（regularization）。正则化通过<strong>为模型损失函数添加惩罚项</strong>使学出的<strong>模型参数值较小</strong>，是应对过拟合的常用手段。</p>\n<h4 id=\"l2-范数正则化（regularization）\">L2 范数正则化（regularization）<a href=\"#l2-范数正则化（regularization）\" title=\"L2 范数正则化（regularization）\"></a></h4><p>带有L2范数惩罚项的新损失函数为</p>\n<p><img src=\"https://i.loli.net/2020/02/15/Pout8nmLbUsZ1gD.png\" class=\"φcy\" alt=\"5\"></p>\n<p>将线性回归一节中权重w1和w2的迭代方式更改为</p>\n<p><img src=\"https://i.loli.net/2020/02/15/YMdVzxKZuqIUFiX.png\" class=\"φcy\" alt=\"6\"></p>\n<p>可见，L2范数正则化令权重 w1 和 w2 先自乘小于1的数，再减去不含惩罚项的梯度。因此，L2范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。</p>\n<h4 id=\"高维线性回归实验从零开始的实现\">高维线性回归实验从零开始的实现<a href=\"#高维线性回归实验从零开始的实现\" title=\"高维线性回归实验从零开始的实现\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">\"/home/kesci/input\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> d2lzh1981 <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"初始化模型参数-1\">初始化模型参数<a href=\"#初始化模型参数-1\" title=\"初始化模型参数\"></a></h4><p>维度是200，样本数20。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n_train, n_test, num_inputs = <span class=\"number\">20</span>, <span class=\"number\">100</span>, <span class=\"number\">200</span></span><br><span class=\"line\">true_w, true_b = torch.ones(num_inputs, <span class=\"number\">1</span>) * <span class=\"number\">0.01</span>, <span class=\"number\">0.05</span></span><br><span class=\"line\"></span><br><span class=\"line\">features = torch.randn((n_train + n_test, num_inputs))</span><br><span class=\"line\">labels = torch.matmul(features, true_w) + true_b</span><br><span class=\"line\">labels += torch.tensor(np.random.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, size=labels.size()), dtype=torch.float)</span><br><span class=\"line\">train_features, test_features = features[:n_train, :], features[n_train:, :]</span><br><span class=\"line\">train_labels, test_labels = labels[:n_train], labels[n_train:]</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义参数初始化函数，初始化模型参数并且附上梯度</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init_params</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    w = torch.randn((num_inputs, <span class=\"number\">1</span>), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    b = torch.zeros(<span class=\"number\">1</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [w, b]</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"定义l2范数惩罚项\">定义L2范数惩罚项<a href=\"#定义l2范数惩罚项\" title=\"定义L2范数惩罚项\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">l2_penalty</span><span class=\"params\">(w)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (w**<span class=\"number\">2</span>).sum() / <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"comment\">#乘一个正数放到后面损失函数再写</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"定义训练和测试\">定义训练和测试<a href=\"#定义训练和测试\" title=\"定义训练和测试\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size, num_epochs, lr = <span class=\"number\">1</span>, <span class=\"number\">100</span>, <span class=\"number\">0.003</span></span><br><span class=\"line\">net, loss = d2l.linreg, d2l.squared_loss</span><br><span class=\"line\"></span><br><span class=\"line\">dataset = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class=\"line\">train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fit_and_plot</span><span class=\"params\">(lambd)</span>:</span></span><br><span class=\"line\">    w, b = init_params()</span><br><span class=\"line\">    train_ls, test_ls = [], []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">            <span class=\"comment\"># 添加了L2范数惩罚项</span></span><br><span class=\"line\">            l = loss(net(X, w, b), y) + lambd * l2_penalty(w)</span><br><span class=\"line\">            l = l.sum()</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"keyword\">if</span> w.grad <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                w.grad.data.zero_()</span><br><span class=\"line\">                b.grad.data.zero_()</span><br><span class=\"line\">            l.backward()</span><br><span class=\"line\">            d2l.sgd([w, b], lr, batch_size)</span><br><span class=\"line\">        train_ls.append(loss(net(train_features, w, b), train_labels).mean().item())</span><br><span class=\"line\">        test_ls.append(loss(net(test_features, w, b), test_labels).mean().item())</span><br><span class=\"line\">    d2l.semilogy(range(<span class=\"number\">1</span>, num_epochs + <span class=\"number\">1</span>), train_ls, <span class=\"string\">'epochs'</span>, <span class=\"string\">'loss'</span>,</span><br><span class=\"line\">                 range(<span class=\"number\">1</span>, num_epochs + <span class=\"number\">1</span>), test_ls, [<span class=\"string\">'train'</span>, <span class=\"string\">'test'</span>])</span><br><span class=\"line\">    print(<span class=\"string\">'L2 norm of w:'</span>, w.norm().item())</span><br></pre></td></tr></table></figure>\n\n<ul><li><p><code>fit_and_plot(lambd)</code>，<code>lambd</code>是人为定义的超参数，就是要乘的正数。</p></li><li><p><code>l = loss(net(X, w, b), y) + lambd * l2_penalty(w)</code> 求完损失要加L2范数惩罚项。</p></li></ul><h4 id=\"观察过拟合\">观察过拟合<a href=\"#观察过拟合\" title=\"观察过拟合\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#lambd等于0就是不添加惩罚项</span></span><br><span class=\"line\">fit_and_plot(lambd=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://i.loli.net/2020/02/15/mUtcNuR4sFxzwk1.png\" class=\"φcy\" alt=\"7\"></p>\n<h4 id=\"使用权重衰减\">使用权重衰减<a href=\"#使用权重衰减\" title=\"使用权重衰减\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fit_and_plot(lambd=<span class=\"number\">3</span>)</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://i.loli.net/2020/02/15/dCw4E1LrYSIiv9u.png\" class=\"φcy\" alt=\"8\"></p>\n<h4 id=\"简洁实现\">简洁实现<a href=\"#简洁实现\" title=\"简洁实现\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">fit_and_plot_pytorch</span><span class=\"params\">(wd)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># 对权重参数衰减。权重名称一般是以weight结尾</span></span><br><span class=\"line\">    net = nn.Linear(num_inputs, <span class=\"number\">1</span>)</span><br><span class=\"line\">    nn.init.normal_(net.weight, mean=<span class=\"number\">0</span>, std=<span class=\"number\">1</span>)</span><br><span class=\"line\">    nn.init.normal_(net.bias, mean=<span class=\"number\">0</span>, std=<span class=\"number\">1</span>)</span><br><span class=\"line\">    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) <span class=\"comment\"># 对权重参数衰减</span></span><br><span class=\"line\">    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  <span class=\"comment\"># 不对偏差参数衰减</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    train_ls, test_ls = [], []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">            l = loss(net(X), y).mean()</span><br><span class=\"line\">            optimizer_w.zero_grad()</span><br><span class=\"line\">            optimizer_b.zero_grad()</span><br><span class=\"line\">            </span><br><span class=\"line\">            l.backward()</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"comment\"># 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差</span></span><br><span class=\"line\">            optimizer_w.step()</span><br><span class=\"line\">            optimizer_b.step()</span><br><span class=\"line\">        train_ls.append(loss(net(train_features), train_labels).mean().item())</span><br><span class=\"line\">        test_ls.append(loss(net(test_features), test_labels).mean().item())</span><br><span class=\"line\">    d2l.semilogy(range(<span class=\"number\">1</span>, num_epochs + <span class=\"number\">1</span>), train_ls, <span class=\"string\">'epochs'</span>, <span class=\"string\">'loss'</span>,</span><br><span class=\"line\">                 range(<span class=\"number\">1</span>, num_epochs + <span class=\"number\">1</span>), test_ls, [<span class=\"string\">'train'</span>, <span class=\"string\">'test'</span>])</span><br><span class=\"line\">    print(<span class=\"string\">'L2 norm of w:'</span>, net.weight.data.norm().item())</span><br></pre></td></tr></table></figure>\n\n<ul><li>参数初始化直接使用<code>nn.init</code></li><li>优化函数直接使用<code>torch.optim.SGD</code>，设置<code>weight_decay=wd</code>启用权重衰减</li></ul><h3 id=\"4-丢弃法\">4. 丢弃法<a href=\"#4-丢弃法\" title=\"4. 丢弃法\"></a></h3><p>举一个例子，</p>\n<p><img src=\"https://i.loli.net/2020/02/15/9GSnBNVWxKw5HzZ.png\" class=\"φcy\" alt=\"9\"></p>\n<p><img src=\"https://i.loli.net/2020/02/15/iGcZTMkSULwNuzI.png\" class=\"φcy\" alt=\"10\"></p>\n<p>简而言之，就是以一定概率丢弃某个隐藏层中的某个单元。不丢弃的单元要除以（1-p）。</p>\n<h4 id=\"丢弃法从零开始的实现\">丢弃法从零开始的实现<a href=\"#丢弃法从零开始的实现\" title=\"丢弃法从零开始的实现\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">\"/home/kesci/input\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> d2lzh1981 <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">dropout</span><span class=\"params\">(X, drop_prob)</span>:</span></span><br><span class=\"line\">    X = X.float()</span><br><span class=\"line\">    <span class=\"keyword\">assert</span> <span class=\"number\">0</span> &lt;= drop_prob &lt;= <span class=\"number\">1</span>\t<span class=\"comment\">#判断丢弃率是否在0-1之间</span></span><br><span class=\"line\">    keep_prob = <span class=\"number\">1</span> - drop_prob</span><br><span class=\"line\">    <span class=\"comment\"># 这种情况下把全部元素都丢弃</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> keep_prob == <span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.zeros_like(X)</span><br><span class=\"line\">    mask = (torch.rand(X.shape) &lt; keep_prob).float()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">return</span> mask * X / keep_prob</span><br></pre></td></tr></table></figure>\n\n<ul><li><code>mask = (torch.rand(X.shape) &lt; keep_prob).float()</code>：先随机生成一个与 X 形状相同的矩阵，里面的元素小于<code>keep_prob</code>的要保留，即把这个值制1，大于<code>keep_prob</code>的要丢弃，这个位置就为填0.</li><li><code>mask * X / keep_prob</code>：把这个含01的矩阵mask与X相乘，矩阵X的某些位置变成了0，不为0的位置除以<code>keep_prob</code>拉伸。</li></ul><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.arange(<span class=\"number\">16</span>).view(<span class=\"number\">2</span>, <span class=\"number\">8</span>)\t<span class=\"comment\"># 定义X</span></span><br><span class=\"line\">dropout(X, <span class=\"number\">0</span>)\t<span class=\"comment\">#不丢弃</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">输出：</span><br><span class=\"line\">tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],</span><br><span class=\"line\">        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dropout(X, 0.5)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">输出：</span><br><span class=\"line\">tensor([[ 0.,  0.,  0.,  6.,  8., 10.,  0., 14.],</span><br><span class=\"line\">        [ 0.,  0., 20.,  0.,  0.,  0., 28.,  0.]])</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dropout(X, <span class=\"number\">1.0</span>)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>],</span><br><span class=\"line\">        [<span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>, <span class=\"number\">0.</span>]])</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 参数的初始化</span></span><br><span class=\"line\">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class=\"number\">784</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span>, <span class=\"number\">256</span></span><br><span class=\"line\"></span><br><span class=\"line\">W1 = torch.tensor(np.random.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">b1 = torch.zeros(num_hiddens1, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">W2 = torch.tensor(np.random.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">b2 = torch.zeros(num_hiddens2, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">W3 = torch.tensor(np.random.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">b3 = torch.zeros(num_outputs, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">params = [W1, b1, W2, b2, W3, b3]</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">drop_prob1, drop_prob2 = <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span>\t<span class=\"comment\">#有两个隐藏层</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">net</span><span class=\"params\">(X, is_training=True)</span>:</span></span><br><span class=\"line\">    X = X.view(<span class=\"number\">-1</span>, num_inputs)</span><br><span class=\"line\">    H1 = (torch.matmul(X, W1) + b1).relu()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> is_training:  <span class=\"comment\"># 只在训练模型时使用丢弃法</span></span><br><span class=\"line\">        H1 = dropout(H1, drop_prob1)  <span class=\"comment\"># 在第一层全连接后添加丢弃层</span></span><br><span class=\"line\">    H2 = (torch.matmul(H1, W2) + b2).relu()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> is_training:</span><br><span class=\"line\">        H2 = dropout(H2, drop_prob2)  <span class=\"comment\"># 在第二层全连接后添加丢弃层</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(H2, W3) + b3</span><br></pre></td></tr></table></figure>\n\n<ul><li><code>is_training=True</code>：训练时需要丢弃，测试时不需要丢弃。</li></ul><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">evaluate_accuracy</span><span class=\"params\">(data_iter, net)</span>:</span></span><br><span class=\"line\">    acc_sum, n = <span class=\"number\">0.0</span>, <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">        <span class=\"keyword\">if</span> isinstance(net, torch.nn.Module):</span><br><span class=\"line\">            net.eval() <span class=\"comment\"># 评估模式, 这会关闭dropout</span></span><br><span class=\"line\">            acc_sum += (net(X).argmax(dim=<span class=\"number\">1</span>) == y).float().sum().item()</span><br><span class=\"line\">            net.train() <span class=\"comment\"># 改回训练模式</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>: <span class=\"comment\"># 自定义的模型</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span>(<span class=\"string\">'is_training'</span> <span class=\"keyword\">in</span> net.__code__.co_varnames): <span class=\"comment\"># 如果有is_training这个参数</span></span><br><span class=\"line\">                <span class=\"comment\"># 将is_training设置成False</span></span><br><span class=\"line\">                acc_sum += (net(X, is_training=<span class=\"literal\">False</span>).argmax(dim=<span class=\"number\">1</span>) == y).float().sum().item() </span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                acc_sum += (net(X).argmax(dim=<span class=\"number\">1</span>) == y).float().sum().item() </span><br><span class=\"line\">        n += y.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> acc_sum / n</span><br></pre></td></tr></table></figure>\n\n<ul><li>先判断这个net是否是<code>torch.nn.Module</code>，是的话就使用<code>net.eval()</code> 评估模式，关闭dropout丢弃法。</li></ul><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, lr, batch_size = <span class=\"number\">5</span>, <span class=\"number\">100.0</span>, <span class=\"number\">256</span>  <span class=\"comment\"># 这里的学习率设置的很大，原因与之前相同。</span></span><br><span class=\"line\">loss = torch.nn.CrossEntropyLoss()</span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=<span class=\"string\">'/home/kesci/input/FashionMNIST2065'</span>)</span><br><span class=\"line\">d2l.train_ch3(</span><br><span class=\"line\">    net,</span><br><span class=\"line\">    train_iter,</span><br><span class=\"line\">    test_iter,</span><br><span class=\"line\">    loss,</span><br><span class=\"line\">    num_epochs,</span><br><span class=\"line\">    batch_size,</span><br><span class=\"line\">    params,</span><br><span class=\"line\">    lr)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 1, loss 0.0046, train acc 0.541, test acc 0.656</span><br><span class=\"line\">epoch 2, loss 0.0023, train acc 0.783, test acc 0.784</span><br><span class=\"line\">epoch 3, loss 0.0019, train acc 0.821, test acc 0.760</span><br><span class=\"line\">epoch 4, loss 0.0017, train acc 0.838, test acc 0.832</span><br><span class=\"line\">epoch 5, loss 0.0016, train acc 0.849, test acc 0.836</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"简洁实现：\">简洁实现：<a href=\"#简洁实现：\" title=\"简洁实现：\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">        d2l.FlattenLayer(),\t<span class=\"comment\">#改输入格式</span></span><br><span class=\"line\">        nn.Linear(num_inputs, num_hiddens1),\t<span class=\"comment\">#隐藏层</span></span><br><span class=\"line\">        nn.ReLU(),\t<span class=\"comment\">#激活函数</span></span><br><span class=\"line\">        nn.Dropout(drop_prob1),\t<span class=\"comment\">#丢弃法</span></span><br><span class=\"line\">        nn.Linear(num_hiddens1, num_hiddens2), </span><br><span class=\"line\">        nn.ReLU(),</span><br><span class=\"line\">        nn.Dropout(drop_prob2),</span><br><span class=\"line\">        nn.Linear(num_hiddens2, <span class=\"number\">10</span>)</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> net.parameters():</span><br><span class=\"line\">    nn.init.normal_(param, mean=<span class=\"number\">0</span>, std=<span class=\"number\">0.01</span>)\t<span class=\"comment\">#初始化参数</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">optimizer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.5</span>)\t<span class=\"comment\">#优化网络中的参数</span></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class=\"literal\">None</span>, <span class=\"literal\">None</span>, optimizer)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 1, loss 0.0045, train acc 0.550, test acc 0.739</span><br><span class=\"line\">epoch 2, loss 0.0023, train acc 0.783, test acc 0.746</span><br><span class=\"line\">epoch 3, loss 0.0019, train acc 0.819, test acc 0.727</span><br><span class=\"line\">epoch 4, loss 0.0018, train acc 0.831, test acc 0.806</span><br><span class=\"line\">epoch 5, loss 0.0016, train acc 0.847, test acc 0.787</span><br></pre></td></tr></table></figure>\n\n","prev":{"title":"pytorch笔记五：文本预处理","link":"2020/02/16/pytorch笔记五：文本预处理"},"next":{"title":"pytorch笔记三：多层感知机","link":"2020/02/14/pytorch笔记三：多层感知机"},"plink":"https://yuyuoo.github.io/2020/02/14/pytorch笔记四：过拟合欠拟合及其解决方案/","toc":[{"id":"1-模型选择、过拟合和欠拟合","title":"1. 模型选择、过拟合和欠拟合","index":"1"},{"id":"2-多项式函数拟合实验","title":"2. 多项式函数拟合实验","index":"2"},{"id":"3-权重衰减","title":"3. 权重衰减","index":"3"},{"id":"4-丢弃法","title":"4. 丢弃法","index":"4"}],"copyright":{"license":"Attribution-NonCommercial-NoDerivatives 4.0 International","published":"February 14, 2020","updated":"March 7, 2020","author":"YuYuoo"}}