{"title":"pytorch笔记二：Softmax与分类模型","date":"2020-02-13T15:58:37.000Z","date_formatted":{"ll":"Feb 13, 2020","L":"02/13/2020","MM-DD":"02-13"},"thumbnail":"https://i.loli.net/2020/02/14/XAa7gmMFTe1JlLn.png","link":"2020/02/13/pytorch笔记二：Softmax与分类模型","comments":true,"tags":["pytorch"],"categories":["Notes"],"updated":"2020-03-07T06:40:31.531Z","content":"<p>Softmax是对离散数据分类，这里的例子是图像分类。</p>\n<p>使用softmax运算符，使得输出的几个值都为正数且和为1，把预测概率最大的类别作为输出类别。</p>\n<p>使用交叉熵损失函数。</p>\n<h3 id=\"1-获取fashion-mnist训练集和读取数据\">1. 获取Fashion-MNIST训练集和读取数据<a href=\"#1-获取fashion-mnist训练集和读取数据\" title=\"1. 获取Fashion-MNIST训练集和读取数据\"></a></h3><p>这里我们会使用torchvision包，它是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。torchvision主要由以下几部分构成：</p>\n<ol><li>torchvision.datasets: 一些加载数据的函数及常用的数据集接口；</li><li>torchvision.models: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；</li><li>torchvision.transforms: 常用的图片变换，例如裁剪、旋转等；</li><li>torchvision.utils: 其他的一些有用的方法。</li></ol><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># import needed package</span></span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"><span class=\"keyword\">from</span> IPython <span class=\"keyword\">import</span> display</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> transforms</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">\"/home/kesci/input\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> d2lzh1981 <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"get-dataset\">get dataset<a href=\"#get-dataset\" title=\"get dataset\"></a></h5><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mnist_train = torchvision.datasets.FashionMNIST(root=<span class=\"string\">'/home/kesci/input/FashionMNIST2065'</span>, train=<span class=\"literal\">True</span>, download=<span class=\"literal\">True</span>, transform=transforms.ToTensor())</span><br><span class=\"line\">mnist_test = torchvision.datasets.FashionMNIST(root=<span class=\"string\">'/home/kesci/input/FashionMNIST2065'</span>, train=<span class=\"literal\">False</span>, download=<span class=\"literal\">True</span>, transform=transforms.ToTensor())</span><br></pre></td></tr></table></figure>\n\n<p><code>class torchvision.datasets.FashionMNIST(root, train=True, transform=None, target_transform=None, download=False)</code></p>\n<ul><li><code>root</code>（string）– 数据集的根目录，其中存放processed/training.pt和processed/test.pt文件。</li><li><code>train</code>（bool, 可选）– 如果设置为True，从training.pt创建数据集，否则从test.pt创建。</li><li><code>download</code>（bool, 可选）– 如果设置为True，从互联网下载数据并放到root文件夹下。如果root目录下已经存在数据，不会再次下载。</li><li><code>transform</code>（可被调用 , 可选）– 一种函数或变换，输入PIL图片，返回变换之后的数据。如：transforms.RandomCrop。</li><li><code>target_transform</code>（可被调用 , 可选）– 一种函数或变换，输入目标，进行变换。</li></ul><p>看一下下载到的数据：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># show result </span></span><br><span class=\"line\">print(type(mnist_train))\t<span class=\"comment\">#&lt;class 'torchvision.datasets.mnist.FashionMNIST'&gt;</span></span><br><span class=\"line\">print(len(mnist_train), len(mnist_test))\t<span class=\"comment\">#60000 10000</span></span><br></pre></td></tr></table></figure>\n\n<p>通过下标来访问任意一个样本，如<code>feature, label = mnist_train[0]</code></p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 本函数已保存在d2lzh包中方便以后使用，把标签转为文本信息</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_fashion_mnist_labels</span><span class=\"params\">(labels)</span>:</span></span><br><span class=\"line\">    text_labels = [<span class=\"string\">'t-shirt'</span>, <span class=\"string\">'trouser'</span>, <span class=\"string\">'pullover'</span>, <span class=\"string\">'dress'</span>, <span class=\"string\">'coat'</span>,</span><br><span class=\"line\">                   <span class=\"string\">'sandal'</span>, <span class=\"string\">'shirt'</span>, <span class=\"string\">'sneaker'</span>, <span class=\"string\">'bag'</span>, <span class=\"string\">'ankle boot'</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> [text_labels[int(i)] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> labels]</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">show_fashion_mnist</span><span class=\"params\">(images, labels)</span>:</span></span><br><span class=\"line\">    d2l.use_svg_display()</span><br><span class=\"line\">    <span class=\"comment\"># 这里的_表示我们忽略（不使用）的变量</span></span><br><span class=\"line\">    _, figs = plt.subplots(<span class=\"number\">1</span>, len(images), figsize=(<span class=\"number\">12</span>, <span class=\"number\">12</span>))</span><br><span class=\"line\">    <span class=\"keyword\">for</span> f, img, lbl <span class=\"keyword\">in</span> zip(figs, images, labels):</span><br><span class=\"line\">        f.imshow(img.view((<span class=\"number\">28</span>, <span class=\"number\">28</span>)).numpy())</span><br><span class=\"line\">        f.set_title(lbl)</span><br><span class=\"line\">        f.axes.get_xaxis().set_visible(<span class=\"literal\">False</span>)</span><br><span class=\"line\">        f.axes.get_yaxis().set_visible(<span class=\"literal\">False</span>)</span><br><span class=\"line\">    plt.show()</span><br></pre></td></tr></table></figure>\n\n<p>下面做数据集的展示</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X, y = [], []</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">10</span>):</span><br><span class=\"line\">    X.append(mnist_train[i][<span class=\"number\">0</span>]) <span class=\"comment\"># 将第i个feature加到X中</span></span><br><span class=\"line\">    y.append(mnist_train[i][<span class=\"number\">1</span>]) <span class=\"comment\"># 将第i个label加到y中</span></span><br><span class=\"line\">show_fashion_mnist(X, get_fashion_mnist_labels(y))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://i.loli.net/2020/02/14/gjKdevmqVDUZOCs.png\" class=\"φcy\" alt=\"数据集\"></p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 读取数据</span></span><br><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">num_workers = <span class=\"number\">4</span>\t<span class=\"comment\">#工作线程</span></span><br><span class=\"line\">train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=<span class=\"literal\">True</span>, num_workers=num_workers)</span><br><span class=\"line\">test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=<span class=\"literal\">False</span>, num_workers=num_workers)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"2-softmax从零开始的实现\">2. softmax从零开始的实现<a href=\"#2-softmax从零开始的实现\" title=\"2. softmax从零开始的实现\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys\t<span class=\"comment\">#是为了加载下面d2lzh1981模块</span></span><br><span class=\"line\">sys.path.append(<span class=\"string\">\"/home/kesci/input\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> d2lzh1981 <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=<span class=\"string\">'/home/kesci/input/FashionMNIST2065'</span>)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"模型参数初始化\">模型参数初始化<a href=\"#模型参数初始化\" title=\"模型参数初始化\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs = <span class=\"number\">784</span>\t<span class=\"comment\">#图像像素是28*28</span></span><br><span class=\"line\">num_outputs = <span class=\"number\">10</span>\t<span class=\"comment\">#输出是10种类别</span></span><br><span class=\"line\"></span><br><span class=\"line\">W = torch.tensor(np.random.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, (num_inputs, num_outputs)), dtype=torch.float)</span><br><span class=\"line\">b = torch.zeros(num_outputs, dtype=torch.float)</span><br></pre></td></tr></table></figure>\n\n<p>给权重和偏差附加梯度，方便后面反向传播</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">W.requires_grad_(requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">b.requires_grad_(requires_grad=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"对多维tensor按维度操作举例\">对多维Tensor按维度操作举例<a href=\"#对多维tensor按维度操作举例\" title=\"对多维Tensor按维度操作举例\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X = torch.tensor([[<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>], [<span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">6</span>]])</span><br><span class=\"line\">print(X.sum(dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">True</span>))  <span class=\"comment\"># dim为0，按照相同的列求和，并在结果中保留列特征</span></span><br><span class=\"line\">print(X.sum(dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>))  <span class=\"comment\"># dim为1，按照相同的行求和，并在结果中保留行特征</span></span><br><span class=\"line\">print(X.sum(dim=<span class=\"number\">0</span>, keepdim=<span class=\"literal\">False</span>)) <span class=\"comment\"># dim为0，按照相同的列求和，不在结果中保留列特征</span></span><br><span class=\"line\">print(X.sum(dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">False</span>)) <span class=\"comment\"># dim为1，按照相同的行求和，不在结果中保留行特征</span></span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[<span class=\"number\">5</span>, <span class=\"number\">7</span>, <span class=\"number\">9</span>]])</span><br><span class=\"line\">tensor([[ <span class=\"number\">6</span>],</span><br><span class=\"line\">        [<span class=\"number\">15</span>]])\t<span class=\"comment\">#依然是两行</span></span><br><span class=\"line\">tensor([<span class=\"number\">5</span>, <span class=\"number\">7</span>, <span class=\"number\">9</span>])</span><br><span class=\"line\">tensor([ <span class=\"number\">6</span>, <span class=\"number\">15</span>])\t<span class=\"comment\">#这里不是两行了</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"定义softmax操作\">定义softmax操作<a href=\"#定义softmax操作\" title=\"定义softmax操作\"></a></h4><p><img src=\"https://i.loli.net/2020/02/14/JSk8r9T5vOQWn4X.png\" class=\"φcy\" alt=\"sofamax操作\"></p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">softmax</span><span class=\"params\">(X)</span>:</span></span><br><span class=\"line\">    X_exp = X.exp()\t<span class=\"comment\">#求X的指数</span></span><br><span class=\"line\">    partition = X_exp.sum(dim=<span class=\"number\">1</span>, keepdim=<span class=\"literal\">True</span>)\t<span class=\"comment\">#指数求和</span></span><br><span class=\"line\">    <span class=\"comment\"># print(\"X size is \", X_exp.size())</span></span><br><span class=\"line\">    <span class=\"comment\"># print(\"partition size is \", partition, partition.size())</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> X_exp / partition  <span class=\"comment\"># 这里应用了广播机制</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"softmax回归模型\">softmax回归模型<a href=\"#softmax回归模型\" title=\"softmax回归模型\"></a></h4><p><img src=\"https://i.loli.net/2020/02/14/pvmN4OYnQh3loxz.png\" class=\"φcy\" alt=\"softmax回归模型\"></p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">net</span><span class=\"params\">(X)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> softmax(torch.mm(X.view((<span class=\"number\">-1</span>, num_inputs)), W) + b)</span><br></pre></td></tr></table></figure>\n\n<ul><li><code>X.view</code>使 X 变形，才能与 W 相乘。 W 是784*10</li></ul><h4 id=\"定义损失函数\">定义损失函数<a href=\"#定义损失函数\" title=\"定义损失函数\"></a></h4><p><img src=\"https://i.loli.net/2020/02/14/i5jqaZkVbO9ph3m.png\" class=\"φcy\" alt=\"损失函数\"></p>\n<p>把第一个式子代入第二个，得到第三个。第三个式子是最后要应用的公式。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y_hat = torch.tensor([[<span class=\"number\">0.1</span>, <span class=\"number\">0.3</span>, <span class=\"number\">0.6</span>], [<span class=\"number\">0.3</span>, <span class=\"number\">0.2</span>, <span class=\"number\">0.5</span>]])</span><br><span class=\"line\">y = torch.LongTensor([<span class=\"number\">0</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">y_hat.gather(<span class=\"number\">1</span>, y.view(<span class=\"number\">-1</span>, <span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n<ul><li>gather()取数据，第一个参数1是 dimension，表示按行取；第二个参数把 y 变成2行1列的tensor，那么就要到 y_hat 中第一行取第0个，第二行取第2个。取出0.1和0.5。</li></ul><p>交叉熵损失函数：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">cross_entropy</span><span class=\"params\">(y_hat, y)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> - torch.log(y_hat.gather(<span class=\"number\">1</span>, y.view(<span class=\"number\">-1</span>, <span class=\"number\">1</span>)))</span><br></pre></td></tr></table></figure>\n\n<ul><li>y_hat 是 y 的估计，从中按 y 的数字取。</li></ul><h4 id=\"定义准确率\">定义准确率<a href=\"#定义准确率\" title=\"定义准确率\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">accuracy</span><span class=\"params\">(y_hat, y)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (y_hat.argmax(dim=<span class=\"number\">1</span>) == y).float().mean().item()</span><br></pre></td></tr></table></figure>\n\n<ul><li>取 y_hat 的最大的一个值，比较是否和真实值 y 相等，相等就为1，否则为0。把这些1和0取平均值，得到准确率。</li></ul><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 本函数已保存在d2lzh_pytorch包中方便以后使用。该函数将被逐步改进：它的完整实现将在“图像增广”一节中描述</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">evaluate_accuracy</span><span class=\"params\">(data_iter, net)</span>:</span></span><br><span class=\"line\">    acc_sum, n = <span class=\"number\">0.0</span>, <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">        acc_sum += (net(X).argmax(dim=<span class=\"number\">1</span>) == y).float().sum().item()</span><br><span class=\"line\">        n += y.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">    <span class=\"keyword\">return</span> acc_sum / n</span><br></pre></td></tr></table></figure>\n\n<ul><li>所有的准确率累加，除以样本数。</li></ul><h4 id=\"训练模型\">训练模型<a href=\"#训练模型\" title=\"训练模型\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, lr = <span class=\"number\">5</span>, <span class=\"number\">0.1</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 本函数已保存在d2lzh_pytorch包中方便以后使用</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_ch3</span><span class=\"params\">(net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">              params=None, lr=None, optimizer=None)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">        train_l_sum, train_acc_sum, n = <span class=\"number\">0.0</span>, <span class=\"number\">0.0</span>, <span class=\"number\">0</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> train_iter:</span><br><span class=\"line\">            y_hat = net(X)</span><br><span class=\"line\">            l = loss(y_hat, y).sum()</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"comment\"># 梯度清零</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> optimizer <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                optimizer.zero_grad()</span><br><span class=\"line\">            <span class=\"keyword\">elif</span> params <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span> <span class=\"keyword\">and</span> params[<span class=\"number\">0</span>].grad <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">                    param.grad.data.zero_()</span><br><span class=\"line\">            </span><br><span class=\"line\">            l.backward()</span><br><span class=\"line\">            <span class=\"keyword\">if</span> optimizer <span class=\"keyword\">is</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                d2l.sgd(params, lr, batch_size)</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                optimizer.step() </span><br><span class=\"line\">            </span><br><span class=\"line\">            </span><br><span class=\"line\">            train_l_sum += l.item()</span><br><span class=\"line\">            train_acc_sum += (y_hat.argmax(dim=<span class=\"number\">1</span>) == y).sum().item()</span><br><span class=\"line\">            n += y.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class=\"line\">        print(<span class=\"string\">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class=\"line\">              % (epoch + <span class=\"number\">1</span>, train_l_sum / n, train_acc_sum / n, test_acc))</span><br><span class=\"line\"></span><br><span class=\"line\">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)</span><br></pre></td></tr></table></figure>\n\n<p>输出：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 1, loss 0.7851, train acc 0.750, test acc 0.791</span><br><span class=\"line\">epoch 2, loss 0.5704, train acc 0.814, test acc 0.810</span><br><span class=\"line\">epoch 3, loss 0.5258, train acc 0.825, test acc 0.819</span><br><span class=\"line\">epoch 4, loss 0.5014, train acc 0.832, test acc 0.824</span><br><span class=\"line\">epoch 5, loss 0.4865, train acc 0.836, test acc 0.827</span><br></pre></td></tr></table></figure>\n\n<ul><li>optimizer是可选的，有 optimizer 时很多操作都省了。</li></ul><h4 id=\"模型预测\">模型预测<a href=\"#模型预测\" title=\"模型预测\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">X, y &#x3D; iter(test_iter).next()</span><br><span class=\"line\"></span><br><span class=\"line\">true_labels &#x3D; d2l.get_fashion_mnist_labels(y.numpy())</span><br><span class=\"line\">pred_labels &#x3D; d2l.get_fashion_mnist_labels(net(X).argmax(dim&#x3D;1).numpy())</span><br><span class=\"line\">titles &#x3D; [true + &#39;\\n&#39; + pred for true, pred in zip(true_labels, pred_labels)]</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.show_fashion_mnist(X[0:9], titles[0:9])</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://i.loli.net/2020/02/14/TLX17kjehKAHBgM.png\" class=\"φcy\" alt=\"模型预测\"></p>\n<h3 id=\"3-softmax的简洁实现\">3. softmax的简洁实现<a href=\"#3-softmax的简洁实现\" title=\"3. softmax的简洁实现\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 加载各种包或者模块</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> init</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">\"/home/kesci/input\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> d2lzh1981 <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"初始化参数和获取数据\">初始化参数和获取数据<a href=\"#初始化参数和获取数据\" title=\"初始化参数和获取数据\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root=<span class=\"string\">'/home/kesci/input/FashionMNIST2065'</span>)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"定义网络模型\">定义网络模型<a href=\"#定义网络模型\" title=\"定义网络模型\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs = <span class=\"number\">784</span></span><br><span class=\"line\">num_outputs = <span class=\"number\">10</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">LinearNet</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, num_inputs, num_outputs)</span>:</span></span><br><span class=\"line\">        super(LinearNet, self).__init__()</span><br><span class=\"line\">        self.linear = nn.Linear(num_inputs, num_outputs)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span> <span class=\"comment\"># x 的形状: (batch, 1, 28, 28)</span></span><br><span class=\"line\">        y = self.linear(x.view(x.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> y</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"comment\"># net = LinearNet(num_inputs, num_outputs)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">FlattenLayer</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        super(FlattenLayer, self).__init__()</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, x)</span>:</span> <span class=\"comment\"># x 的形状: (batch, *, *, ...)</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> x.view(x.shape[<span class=\"number\">0</span>], <span class=\"number\">-1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> collections <span class=\"keyword\">import</span> OrderedDict</span><br><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">        <span class=\"comment\"># FlattenLayer(),</span></span><br><span class=\"line\">        <span class=\"comment\"># LinearNet(num_inputs, num_outputs) </span></span><br><span class=\"line\">        OrderedDict([</span><br><span class=\"line\">           (<span class=\"string\">'flatten'</span>, FlattenLayer()),</span><br><span class=\"line\">           (<span class=\"string\">'linear'</span>, nn.Linear(num_inputs, num_outputs))]) <span class=\"comment\"># 或者写成我们自己定义的 LinearNet(num_inputs, num_outputs) 也可以</span></span><br><span class=\"line\">        )</span><br></pre></td></tr></table></figure>\n\n<ul><li>有两个层，第一个层是转换 x 的形状，第二个层是线性层。</li></ul><h4 id=\"初始化模型参数\">初始化模型参数<a href=\"#初始化模型参数\" title=\"初始化模型参数\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">init.normal_(net.linear.weight, mean=<span class=\"number\">0</span>, std=<span class=\"number\">0.01</span>)</span><br><span class=\"line\">init.constant_(net.linear.bias, val=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"定义损失函数-1\">定义损失函数<a href=\"#定义损失函数-1\" title=\"定义损失函数\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss = nn.CrossEntropyLoss() <span class=\"comment\"># 下面是他的函数原型</span></span><br><span class=\"line\"><span class=\"comment\"># class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"定义优化函数\">定义优化函数<a href=\"#定义优化函数\" title=\"定义优化函数\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">optimizer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.1</span>) <span class=\"comment\"># 下面是函数原型</span></span><br><span class=\"line\"><span class=\"comment\"># class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)</span></span><br></pre></td></tr></table></figure>\n\n<ul><li>构建好神经网络后，网络的参数都保存在 parameters () 函数当中</li></ul><h4 id=\"训练\">训练<a href=\"#训练\" title=\"训练\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs = <span class=\"number\">5</span></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class=\"literal\">None</span>, <span class=\"literal\">None</span>, optimizer)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 1, loss 0.0031, train acc 0.751, test acc 0.795</span><br><span class=\"line\">epoch 2, loss 0.0022, train acc 0.813, test acc 0.809</span><br><span class=\"line\">epoch 3, loss 0.0021, train acc 0.825, test acc 0.806</span><br><span class=\"line\">epoch 4, loss 0.0020, train acc 0.833, test acc 0.813</span><br><span class=\"line\">epoch 5, loss 0.0019, train acc 0.837, test acc 0.822</span><br></pre></td></tr></table></figure>\n\n<p>在刚开始训练时，训练数据集上的准确率低于测试数据集上的准确率，原因是：</p>\n<p>训练集上的准确率是在一个 epoch 的过程中计算得到的，测试集上的准确率是在一个 epoch 结束后计算得到的，后者的模型参数更优。</p>\n","prev":{"title":"pytorch笔记三：多层感知机","link":"2020/02/14/pytorch笔记三：多层感知机"},"next":{"title":"pytorch笔记一：线性回归","link":"2020/02/13/pytorch笔记一：线性回归"},"plink":"https://yuyuoo.github.io/2020/02/13/pytorch笔记二：Softmax与分类模型/","toc":[{"id":"1-获取fashion-mnist训练集和读取数据","title":"1. 获取Fashion-MNIST训练集和读取数据","index":"1"},{"id":"2-softmax从零开始的实现","title":"2. softmax从零开始的实现","index":"2"},{"id":"3-softmax的简洁实现","title":"3. softmax的简洁实现","index":"3"}],"copyright":{"license":"Attribution-NonCommercial-NoDerivatives 4.0 International","published":"February 13, 2020","updated":"March 7, 2020","author":"YuYuoo"}}