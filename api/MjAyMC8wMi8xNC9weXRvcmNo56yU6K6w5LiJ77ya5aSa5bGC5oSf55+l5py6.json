{"title":"pytorch笔记三：多层感知机","date":"2020-02-14T06:08:00.000Z","date_formatted":{"ll":"Feb 14, 2020","L":"02/14/2020","MM-DD":"02-14"},"thumbnail":"https://i.loli.net/2020/02/14/liZIcysfJVmzDv8.jpg","link":"2020/02/14/pytorch笔记三：多层感知机","comments":true,"tags":["pytorch"],"categories":["Notes"],"updated":"2020-03-07T06:40:41.480Z","content":"<p>简单的添加隐藏层仍然等价于一个单层神经网络，只不过式子复杂一些，比如</p>\n<p><img src=\"https://i.loli.net/2020/02/14/h3kzHuvQoDSLNYg.png\" class=\"φcy\" alt=\"1\"></p>\n<h4 id=\"激活函数\">激活函数<a href=\"#激活函数\" title=\"激活函数\"></a></h4><p>为了解决这个问题，引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。</p>\n<p>下面介绍几个常用的激活函数：</p>\n<h5 id=\"relu函数\">ReLU函数<a href=\"#relu函数\" title=\"ReLU函数\"></a></h5><p>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素x，该函数定义为</p>\n<p>ReLU(x)=max(x,0). 也就是只保留正数元素，并将负数元素清零。</p>\n<p><img src=\"https://i.loli.net/2020/02/14/I4o9DwYpRM3dNyj.png\" class=\"φcy\" alt=\"2\"></p>\n<h5 id=\"sigmoid函数\">Sigmoid函数<a href=\"#sigmoid函数\" title=\"Sigmoid函数\"></a></h5><p>sigmoid函数可以将元素的值变换到0和1之间：</p>\n<p><img src=\"https://i.loli.net/2020/02/14/dn5uXZ24MjzheTS.png\" class=\"φcy\" alt=\"3\"></p>\n<h5 id=\"tanh函数\">tanh函数<a href=\"#tanh函数\" title=\"tanh函数\"></a></h5><p>tanh（双曲正切）函数可以将元素的值变换到-1和1之间：</p>\n<p><img src=\"https://i.loli.net/2020/02/14/yorwF5CBlQHMi9n.png\" class=\"φcy\" alt=\"4\"></p>\n<h5 id=\"关于激活函数的选择\">关于激活函数的选择<a href=\"#关于激活函数的选择\" title=\"关于激活函数的选择\"></a></h5><p>ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。</p>\n<p>用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。</p>\n<p>在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。</p>\n<p>在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。</p>\n<h3 id=\"1-多层感知机从零开始的实现\">1. 多层感知机从零开始的实现<a href=\"#1-多层感知机从零开始的实现\" title=\"1. 多层感知机从零开始的实现\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">\"/home/kesci/input\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> d2lzh1981 <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"获取训练集\">获取训练集<a href=\"#获取训练集\" title=\"获取训练集\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=<span class=\"string\">'/home/kesci/input/FashionMNIST2065'</span>)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"定义模型参数\">定义模型参数<a href=\"#定义模型参数\" title=\"定义模型参数\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs, num_outputs, num_hiddens = <span class=\"number\">784</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#两层网络，初始化4个参数</span></span><br><span class=\"line\">W1 = torch.tensor(np.random.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, (num_inputs, num_hiddens)), dtype=torch.float)</span><br><span class=\"line\">b1 = torch.zeros(num_hiddens, dtype=torch.float)</span><br><span class=\"line\">W2 = torch.tensor(np.random.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, (num_hiddens, num_outputs)), dtype=torch.float)</span><br><span class=\"line\">b2 = torch.zeros(num_outputs, dtype=torch.float)</span><br><span class=\"line\"></span><br><span class=\"line\">params = [W1, b1, W2, b2]</span><br><span class=\"line\"><span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">    param.requires_grad_(requires_grad=<span class=\"literal\">True</span>)\t<span class=\"comment\">#附加梯度</span></span><br></pre></td></tr></table></figure>\n\n<ul><li>num_hiddens 是隐藏层输出的个数</li></ul><h4 id=\"定义激活函数\">定义激活函数<a href=\"#定义激活函数\" title=\"定义激活函数\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">relu</span><span class=\"params\">(X)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.max(input=X, other=torch.tensor(<span class=\"number\">0.0</span>))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"定义网络\">定义网络<a href=\"#定义网络\" title=\"定义网络\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">net</span><span class=\"params\">(X)</span>:</span></span><br><span class=\"line\">    X = X.view((<span class=\"number\">-1</span>, num_inputs))</span><br><span class=\"line\">    H = relu(torch.matmul(X, W1) + b1)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.matmul(H, W2) + b2</span><br></pre></td></tr></table></figure>\n\n<ul><li>矩阵相乘有 <code>torch.mm</code> 和 <code>torch.matmul</code> 两个函数。其中前一个是针对二维矩阵，后一个是高维。当 <code>torch.mm</code> 用于大于二维时将报错。</li></ul><h4 id=\"定义损失函数\">定义损失函数<a href=\"#定义损失函数\" title=\"定义损失函数\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss = torch.nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"训练\">训练<a href=\"#训练\" title=\"训练\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, lr = <span class=\"number\">5</span>, <span class=\"number\">100.0</span></span><br><span class=\"line\"><span class=\"comment\"># def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span><br><span class=\"line\"><span class=\"comment\">#               params=None, lr=None, optimizer=None):</span></span><br><span class=\"line\"><span class=\"comment\">#     for epoch in range(num_epochs):</span></span><br><span class=\"line\"><span class=\"comment\">#         train_l_sum, train_acc_sum, n = 0.0, 0.0, 0</span></span><br><span class=\"line\"><span class=\"comment\">#         for X, y in train_iter:</span></span><br><span class=\"line\"><span class=\"comment\">#             y_hat = net(X)</span></span><br><span class=\"line\"><span class=\"comment\">#             l = loss(y_hat, y).sum()</span></span><br><span class=\"line\"><span class=\"comment\">#             </span></span><br><span class=\"line\"><span class=\"comment\">#             # 梯度清零</span></span><br><span class=\"line\"><span class=\"comment\">#             if optimizer is not None:</span></span><br><span class=\"line\"><span class=\"comment\">#                 optimizer.zero_grad()</span></span><br><span class=\"line\"><span class=\"comment\">#             elif params is not None and params[0].grad is not None:</span></span><br><span class=\"line\"><span class=\"comment\">#                 for param in params:</span></span><br><span class=\"line\"><span class=\"comment\">#                     param.grad.data.zero_()</span></span><br><span class=\"line\"><span class=\"comment\">#            </span></span><br><span class=\"line\"><span class=\"comment\">#             l.backward()</span></span><br><span class=\"line\"><span class=\"comment\">#             if optimizer is None:</span></span><br><span class=\"line\"><span class=\"comment\">#                 d2l.sgd(params, lr, batch_size)</span></span><br><span class=\"line\"><span class=\"comment\">#             else:</span></span><br><span class=\"line\"><span class=\"comment\">#                 optimizer.step()  # “softmax回归的简洁实现”一节将用到</span></span><br><span class=\"line\"><span class=\"comment\">#             </span></span><br><span class=\"line\"><span class=\"comment\">#             </span></span><br><span class=\"line\"><span class=\"comment\">#             train_l_sum += l.item()</span></span><br><span class=\"line\"><span class=\"comment\">#             train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()</span></span><br><span class=\"line\"><span class=\"comment\">#             n += y.shape[0]</span></span><br><span class=\"line\"><span class=\"comment\">#         test_acc = evaluate_accuracy(test_iter, net)</span></span><br><span class=\"line\"><span class=\"comment\">#         print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class=\"line\"><span class=\"comment\">#               % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))</span></span><br><span class=\"line\"></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 1, loss 0.0030, train acc 0.712, test acc 0.806</span><br><span class=\"line\">epoch 2, loss 0.0019, train acc 0.821, test acc 0.806</span><br><span class=\"line\">epoch 3, loss 0.0017, train acc 0.847, test acc 0.825</span><br><span class=\"line\">epoch 4, loss 0.0015, train acc 0.856, test acc 0.834</span><br><span class=\"line\">epoch 5, loss 0.0015, train acc 0.863, test acc 0.847</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"2-多层感知机pytorch实现\">2. 多层感知机pytorch实现<a href=\"#2-多层感知机pytorch实现\" title=\"2. 多层感知机pytorch实现\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.nn <span class=\"keyword\">import</span> init</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">\"/home/kesci/input\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> d2lzh1981 <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"初始化模型和各个参数\">初始化模型和各个参数<a href=\"#初始化模型和各个参数\" title=\"初始化模型和各个参数\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs, num_outputs, num_hiddens = <span class=\"number\">784</span>, <span class=\"number\">10</span>, <span class=\"number\">256</span></span><br><span class=\"line\">    </span><br><span class=\"line\">net = nn.Sequential(</span><br><span class=\"line\">        d2l.FlattenLayer(),\t<span class=\"comment\">#把X的28*28转换成784</span></span><br><span class=\"line\">        nn.Linear(num_inputs, num_hiddens),</span><br><span class=\"line\">        nn.ReLU(),</span><br><span class=\"line\">        nn.Linear(num_hiddens, num_outputs), </span><br><span class=\"line\">        )</span><br><span class=\"line\">    </span><br><span class=\"line\"><span class=\"keyword\">for</span> params <span class=\"keyword\">in</span> net.parameters():</span><br><span class=\"line\">    init.normal_(params, mean=<span class=\"number\">0</span>, std=<span class=\"number\">0.01</span>)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"训练-1\">训练<a href=\"#训练-1\" title=\"训练\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch_size = <span class=\"number\">256</span></span><br><span class=\"line\">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=<span class=\"string\">'/home/kesci/input/FashionMNIST2065'</span>)</span><br><span class=\"line\">loss = torch.nn.CrossEntropyLoss()</span><br><span class=\"line\"></span><br><span class=\"line\">optimizer = torch.optim.SGD(net.parameters(), lr=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">num_epochs = <span class=\"number\">5</span></span><br><span class=\"line\">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class=\"literal\">None</span>, <span class=\"literal\">None</span>, optimizer)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epoch 1, loss 0.0031, train acc 0.701, test acc 0.774</span><br><span class=\"line\">epoch 2, loss 0.0019, train acc 0.821, test acc 0.806</span><br><span class=\"line\">epoch 3, loss 0.0017, train acc 0.841, test acc 0.805</span><br><span class=\"line\">epoch 4, loss 0.0015, train acc 0.855, test acc 0.834</span><br><span class=\"line\">epoch 5, loss 0.0014, train acc 0.866, test acc 0.840</span><br></pre></td></tr></table></figure>\n\n","prev":{"title":"pytorch笔记四：过拟合欠拟合及其解决方案","link":"2020/02/14/pytorch笔记四：过拟合欠拟合及其解决方案"},"next":{"title":"pytorch笔记二：Softmax与分类模型","link":"2020/02/13/pytorch笔记二：Softmax与分类模型"},"plink":"https://yuyuoo.github.io/2020/02/14/pytorch笔记三：多层感知机/","toc":[{"id":"1-多层感知机从零开始的实现","title":"1. 多层感知机从零开始的实现","index":"1"},{"id":"2-多层感知机pytorch实现","title":"2. 多层感知机pytorch实现","index":"2"}],"copyright":{"license":"Attribution-NonCommercial-NoDerivatives 4.0 International","published":"February 14, 2020","updated":"March 7, 2020","author":"YuYuoo"}}