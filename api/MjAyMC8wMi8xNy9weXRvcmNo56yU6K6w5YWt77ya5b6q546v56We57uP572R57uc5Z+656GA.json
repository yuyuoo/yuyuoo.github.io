{"title":"pytorch笔记六：循环神经网络基础","date":"2020-02-17T02:19:05.000Z","date_formatted":{"ll":"Feb 17, 2020","L":"02/17/2020","MM-DD":"02-17"},"thumbnail":"https://i.loli.net/2020/02/17/B9PRbW5AlKhVrQy.png","link":"2020/02/17/pytorch笔记六：循环神经网络基础","comments":true,"tags":["pytorch"],"categories":["Notes"],"updated":"2020-03-07T06:41:15.945Z","content":"<p><img src=\"https://i.loli.net/2020/02/17/w6JQOG9hAjZElsL.png\" class=\"φcy\" alt=\"1\"></p>\n<p><img src=\"https://i.loli.net/2020/02/17/UgP7Y4jDCAXtrwk.png\" class=\"φcy\" alt=\"2.png\"></p>\n<h3 id=\"从零开始实现循环神经网络\">从零开始实现循环神经网络<a href=\"#从零开始实现循环神经网络\" title=\"从零开始实现循环神经网络\"></a></h3><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> time</span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\">sys.path.append(<span class=\"string\">\"/home/kesci/input\"</span>)</span><br><span class=\"line\"><span class=\"keyword\">import</span> d2l_jay9460 <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\">(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()</span><br><span class=\"line\">device = torch.device(<span class=\"string\">'cuda'</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">'cpu'</span>)</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"one-hot向量\">one-hot向量<a href=\"#one-hot向量\" title=\"one-hot向量\"></a></h4><p>我们需要将字符表示成向量，这里采用one-hot向量。假设词典大小是N，每次字符对应一个从0到N−1的唯一的索引，则该字符的向量是一个长度为N的向量，若字符的索引是i，则该向量的第i个位置为1，其他位置为0。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">one_hot</span><span class=\"params\">(x, n_class, dtype=torch.float32)</span>:</span></span><br><span class=\"line\">    result = torch.zeros(x.shape[<span class=\"number\">0</span>], n_class, dtype=dtype, device=x.device)  <span class=\"comment\"># shape: (n, n_class)</span></span><br><span class=\"line\">    result.scatter_(<span class=\"number\">1</span>, x.long().view(<span class=\"number\">-1</span>, <span class=\"number\">1</span>), <span class=\"number\">1</span>)  <span class=\"comment\"># result[i, x[i, 0]] = 1</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> result</span><br><span class=\"line\">    </span><br><span class=\"line\">x = torch.tensor([<span class=\"number\">0</span>, <span class=\"number\">2</span>])</span><br><span class=\"line\">x_one_hot = one_hot(x, vocab_size)</span><br><span class=\"line\">print(x_one_hot)</span><br><span class=\"line\">print(x_one_hot.shape)</span><br><span class=\"line\">print(x_one_hot.sum(axis=<span class=\"number\">1</span>))</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[1., 0., 0.,  ..., 0., 0., 0.],</span><br><span class=\"line\">        [0., 0., 1.,  ..., 0., 0., 0.]])</span><br><span class=\"line\">torch.Size([2, 1027])</span><br><span class=\"line\">tensor([1., 1.])</span><br></pre></td></tr></table></figure>\n\n<ul><li>x是一个一维的向量，x的每个元素是一个字符的索引。</li><li>n-class是字典的大小。</li><li>dtype指定返回的向量的数值类型。</li><li>假设x的长度是n，最后返回的向量就是n*n_class的矩阵。</li><li><code>result.scatter_</code>：x转换成n*1的向量，第一个参数1表示对result每行填充，依次取x[i,0]作为列坐标，i作为行坐标，将result[]的这个位置上改为1.</li></ul><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">to_onehot</span><span class=\"params\">(X, n_class)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> [one_hot(X[:, i], n_class) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(X.shape[<span class=\"number\">1</span>])]</span><br><span class=\"line\"></span><br><span class=\"line\">X = torch.arange(<span class=\"number\">10</span>).view(<span class=\"number\">2</span>, <span class=\"number\">5</span>)</span><br><span class=\"line\">inputs = to_onehot(X, vocab_size)</span><br><span class=\"line\">print(len(inputs), inputs[<span class=\"number\">0</span>].shape)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">5 torch.Size([2, 1027])</span><br></pre></td></tr></table></figure>\n\n<ul><li>这里的 vocab_size 是从之前写的<code>d2l.load_data_jay_lyrics()</code>直接拿过来的。</li></ul><h4 id=\"初始化模型参数\">初始化模型参数<a href=\"#初始化模型参数\" title=\"初始化模型参数\"></a></h4><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_inputs, num_hiddens, num_outputs = vocab_size, <span class=\"number\">256</span>, vocab_size</span><br><span class=\"line\"><span class=\"comment\"># num_inputs: d</span></span><br><span class=\"line\"><span class=\"comment\"># num_hiddens: h, 隐藏单元的个数是超参数</span></span><br><span class=\"line\"><span class=\"comment\"># num_outputs: q</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">get_params</span><span class=\"params\">()</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">_one</span><span class=\"params\">(shape)</span>:</span></span><br><span class=\"line\">        param = torch.zeros(shape, device=device, dtype=torch.float32)</span><br><span class=\"line\">        nn.init.normal_(param, <span class=\"number\">0</span>, <span class=\"number\">0.01</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> torch.nn.Parameter(param)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 隐藏层参数</span></span><br><span class=\"line\">    W_xh = _one((num_inputs, num_hiddens))</span><br><span class=\"line\">    W_hh = _one((num_hiddens, num_hiddens))</span><br><span class=\"line\">    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device))</span><br><span class=\"line\">    <span class=\"comment\"># 输出层参数</span></span><br><span class=\"line\">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class=\"line\">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device))</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (W_xh, W_hh, b_h, W_hq, b_q)</span><br></pre></td></tr></table></figure>\n\n<ul><li>对于 W_xh 、W_hh 、W_hq 进行随机初始化，对于 b_h 和 b_q 初始化为0</li></ul><h4 id=\"定义模型\">定义模型<a href=\"#定义模型\" title=\"定义模型\"></a></h4><p>函数<code>rnn</code>用循环的方式依次完成循环神经网络每个时间步的计算。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">rnn</span><span class=\"params\">(inputs, state, params)</span>:</span></span><br><span class=\"line\">    <span class=\"comment\"># inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class=\"line\">    W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class=\"line\">    H, = state</span><br><span class=\"line\">    outputs = []</span><br><span class=\"line\">    <span class=\"keyword\">for</span> X <span class=\"keyword\">in</span> inputs:</span><br><span class=\"line\">        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)</span><br><span class=\"line\">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class=\"line\">        outputs.append(Y)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> outputs, (H,)</span><br></pre></td></tr></table></figure>\n\n<ul><li><p>state 是一个元组，虽然这里只有一个状态，是隐藏状态，但后面lstm不止一个，方便代码复用。</p></li><li><p>H 是各个时间步的隐藏状态。Y 是各个时间步的输出。</p></li><li><p>要返回 H 是因为相邻采样中当前的 H 可作为下一个batch状态的初始值。</p></li></ul><p>构造并初始化模型的参数：</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">init_rnn_state</span><span class=\"params\">(batch_size, num_hiddens, device)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure>\n\n<ul><li>num_hiddens 是隐藏单元的个数，也就是前面的h。</li><li>返回的虽然是一个元组，但是只有一个元素，是长度为1，形状是<code>batch_size* num_hiddens</code>的矩阵，值都是0.</li></ul><figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">print(X.shape)</span><br><span class=\"line\">print(num_hiddens)</span><br><span class=\"line\">print(vocab_size)</span><br><span class=\"line\">state = init_rnn_state(X.shape[<span class=\"number\">0</span>], num_hiddens, device)</span><br><span class=\"line\">inputs = to_onehot(X.to(device), vocab_size)</span><br><span class=\"line\">params = get_params()</span><br><span class=\"line\">outputs, state_new = rnn(inputs, state, params)</span><br><span class=\"line\">print(len(inputs), inputs[<span class=\"number\">0</span>].shape)</span><br><span class=\"line\">print(len(outputs), outputs[<span class=\"number\">0</span>].shape)</span><br><span class=\"line\">print(len(state), state[<span class=\"number\">0</span>].shape)</span><br><span class=\"line\">print(len(state_new), state_new[<span class=\"number\">0</span>].shape)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.Size([2, 5])</span><br><span class=\"line\">256</span><br><span class=\"line\">1027</span><br><span class=\"line\">5 torch.Size([2, 1027])</span><br><span class=\"line\">5 torch.Size([2, 1027])</span><br><span class=\"line\">1 torch.Size([2, 256])</span><br><span class=\"line\">1 torch.Size([2, 256])</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"裁剪梯度\">裁剪梯度<a href=\"#裁剪梯度\" title=\"裁剪梯度\"></a></h4><p>循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。裁剪梯度（clip gradient）是一种应对梯度爆炸的方法。假设我们把所有模型参数的梯度拼接成一个向量 g，并设裁剪的阈值是θ。裁剪后的梯度</p>\n<p><img src=\"https://s2.ax1x.com/2020/02/17/3PLxJK.png\" class=\"φcy\" alt=\"3\"></p>\n<p>的L2范数不超过θ。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">grad_clipping</span><span class=\"params\">(params, theta, device)</span>:</span></span><br><span class=\"line\">    norm = torch.tensor([<span class=\"number\">0.0</span>], device=device)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">        norm += (param.grad.data ** <span class=\"number\">2</span>).sum()</span><br><span class=\"line\">    norm = norm.sqrt().item()</span><br><span class=\"line\">    <span class=\"keyword\">if</span> norm &gt; theta:\t<span class=\"comment\">#相除小于1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">            param.grad.data *= (theta / norm)\t<span class=\"comment\">#得到裁剪后的梯度。</span></span><br></pre></td></tr></table></figure>\n\n<ul><li>params 是模型的参数，theta 是阈值。norm 是梯度的L2范数。</li></ul><h4 id=\"定义预测函数\">定义预测函数<a href=\"#定义预测函数\" title=\"定义预测函数\"></a></h4><p>以下函数基于前缀<code>prefix</code>（含有数个字符的字符串）来预测接下来的<code>num_chars</code>个字符。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict_rnn</span><span class=\"params\">(prefix, num_chars, rnn, params, init_rnn_state,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                num_hiddens, vocab_size, device, idx_to_char, char_to_idx)</span>:</span></span><br><span class=\"line\">    state = init_rnn_state(<span class=\"number\">1</span>, num_hiddens, device)</span><br><span class=\"line\">    output = [char_to_idx[prefix[<span class=\"number\">0</span>]]]   <span class=\"comment\"># output记录prefix加上预测的num_chars个字符</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> range(num_chars + len(prefix) - <span class=\"number\">1</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 将上一时间步的输出作为当前时间步的输入</span></span><br><span class=\"line\">        X = to_onehot(torch.tensor([[output[<span class=\"number\">-1</span>]]], device=device), vocab_size)</span><br><span class=\"line\">        <span class=\"comment\"># 计算输出和更新隐藏状态</span></span><br><span class=\"line\">        (Y, state) = rnn(X, state, params)</span><br><span class=\"line\">        <span class=\"comment\"># 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> t &lt; len(prefix) - <span class=\"number\">1</span>:</span><br><span class=\"line\">            output.append(char_to_idx[prefix[t + <span class=\"number\">1</span>]])</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            output.append(Y[<span class=\"number\">0</span>].argmax(dim=<span class=\"number\">1</span>).item())\t<span class=\"comment\">#把预测的Y这个字符的索引添加进output</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">''</span>.join([idx_to_char[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> output])</span><br></pre></td></tr></table></figure>\n\n<p>我们先测试一下<code>predict_rnn</code>函数。我们将根据前缀“分开”创作长度为10个字符（不考虑前缀长度）的一段歌词。因为模型参数为随机值，所以预测结果也是随机的。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">predict_rnn(<span class=\"string\">'分开'</span>, <span class=\"number\">10</span>, rnn, params, init_rnn_state, num_hiddens, vocab_size,</span><br><span class=\"line\">            device, idx_to_char, char_to_idx)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#39;分开濡时食提危踢拆田唱母&#39;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"困惑度\">困惑度<a href=\"#困惑度\" title=\"困惑度\"></a></h4><p>我们通常使用困惑度（perplexity）来评价语言模型的好坏。回忆一下softmax回归一节中交叉熵损失函数的定义。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，</p>\n<ul><li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；</li><li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</li><li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</li></ul><p>显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典大小<code>vocab_size</code>。</p>\n<h4 id=\"定义模型训练函数\">定义模型训练函数<a href=\"#定义模型训练函数\" title=\"定义模型训练函数\"></a></h4><p>跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：</p>\n<ol><li>使用困惑度评价模型。</li><li>在迭代模型参数前裁剪梯度。</li><li>对时序数据采用不同采样方法将导致隐藏状态初始化的不同。</li></ol><p>对于同一个epoch,随着batch_size的增大，模型损失函数关于隐藏变量的梯度传播得更远，计算开销也更大。为了减小计算开销，可以在每个batch开始的时候把隐藏状态从计算图中分离出来。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_and_predict_rnn</span><span class=\"params\">(rnn, get_params, init_rnn_state, num_hiddens,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                          vocab_size, device, corpus_indices, idx_to_char,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                          char_to_idx, is_random_iter, num_epochs, num_steps,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                          lr, clipping_theta, batch_size, pred_period,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                          pred_len, prefixes)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> is_random_iter:</span><br><span class=\"line\">        data_iter_fn = d2l.data_iter_random</span><br><span class=\"line\">    <span class=\"keyword\">else</span>:</span><br><span class=\"line\">        data_iter_fn = d2l.data_iter_consecutive</span><br><span class=\"line\">    params = get_params()</span><br><span class=\"line\">    loss = nn.CrossEntropyLoss()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> is_random_iter:  <span class=\"comment\"># 如使用相邻采样，在epoch开始时初始化隐藏状态</span></span><br><span class=\"line\">            state = init_rnn_state(batch_size, num_hiddens, device)</span><br><span class=\"line\">        l_sum, n, start = <span class=\"number\">0.0</span>, <span class=\"number\">0</span>, time.time()</span><br><span class=\"line\">        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, Y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> is_random_iter:  <span class=\"comment\"># 如使用随机采样，在每个小批量更新前初始化隐藏状态</span></span><br><span class=\"line\">                state = init_rnn_state(batch_size, num_hiddens, device)</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:  <span class=\"comment\"># 否则需要使用detach函数从计算图分离隐藏状态</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> state:</span><br><span class=\"line\">                    s.detach_()</span><br><span class=\"line\">            <span class=\"comment\"># inputs是num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class=\"line\">            inputs = to_onehot(X, vocab_size)</span><br><span class=\"line\">            <span class=\"comment\"># outputs有num_steps个形状为(batch_size, vocab_size)的矩阵</span></span><br><span class=\"line\">            (outputs, state) = rnn(inputs, state, params)</span><br><span class=\"line\">            <span class=\"comment\"># 拼接之后形状为(num_steps * batch_size, vocab_size)</span></span><br><span class=\"line\">            outputs = torch.cat(outputs, dim=<span class=\"number\">0</span>)</span><br><span class=\"line\">            <span class=\"comment\"># Y的形状是(batch_size, num_steps)，转置后再变成形状为</span></span><br><span class=\"line\">            <span class=\"comment\"># (num_steps * batch_size,)的向量，这样跟输出的行一一对应</span></span><br><span class=\"line\">            y = torch.flatten(Y.T)</span><br><span class=\"line\">            <span class=\"comment\"># 使用交叉熵损失计算平均分类误差</span></span><br><span class=\"line\">            l = loss(outputs, y.long())</span><br><span class=\"line\">            </span><br><span class=\"line\">            <span class=\"comment\"># 梯度清0</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> params[<span class=\"number\">0</span>].grad <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                <span class=\"keyword\">for</span> param <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">                    param.grad.data.zero_()</span><br><span class=\"line\">            l.backward()</span><br><span class=\"line\">            grad_clipping(params, clipping_theta, device)  <span class=\"comment\"># 裁剪梯度</span></span><br><span class=\"line\">            d2l.sgd(params, lr, <span class=\"number\">1</span>)  <span class=\"comment\"># 因为误差已经取过均值，梯度不用再做平均</span></span><br><span class=\"line\">            l_sum += l.item() * y.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">            n += y.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % pred_period == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class=\"line\">                epoch + <span class=\"number\">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class=\"line\">            <span class=\"keyword\">for</span> prefix <span class=\"keyword\">in</span> prefixes:</span><br><span class=\"line\">                print(<span class=\"string\">' -'</span>, predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,</span><br><span class=\"line\">                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"训练模型并创作歌词\">训练模型并创作歌词<a href=\"#训练模型并创作歌词\" title=\"训练模型并创作歌词\"></a></h4><p>现在我们可以训练模型了。首先，设置模型超参数。我们将根据前缀“分开”和“不分开”分别创作长度为50个字符（不考虑前缀长度）的一段歌词。我们每过50个迭代周期便根据当前训练的模型创作一段歌词。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class=\"number\">250</span>, <span class=\"number\">35</span>, <span class=\"number\">32</span>, <span class=\"number\">1e2</span>, <span class=\"number\">1e-2</span></span><br><span class=\"line\">pred_period, pred_len, prefixes = <span class=\"number\">50</span>, <span class=\"number\">50</span>, [<span class=\"string\">'分开'</span>, <span class=\"string\">'不分开'</span>]</span><br></pre></td></tr></table></figure>\n\n<p>下面采用随机采样训练模型并创作歌词。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class=\"line\">                      vocab_size, device, corpus_indices, idx_to_char,</span><br><span class=\"line\">                      char_to_idx, <span class=\"literal\">True</span>, num_epochs, num_steps, lr,</span><br><span class=\"line\">                      clipping_theta, batch_size, pred_period, pred_len,</span><br><span class=\"line\">                      prefixes)</span><br></pre></td></tr></table></figure>\n\n<p>结果太智障了就不贴在这里了。</p>\n<p>接下来采用相邻采样训练模型并创作歌词。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</span><br><span class=\"line\">                      vocab_size, device, corpus_indices, idx_to_char,</span><br><span class=\"line\">                      char_to_idx, <span class=\"literal\">False</span>, num_epochs, num_steps, lr,</span><br><span class=\"line\">                      clipping_theta, batch_size, pred_period, pred_len,</span><br><span class=\"line\">                      prefixes)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"循环神经网络的简洁实现\">循环神经网络的简洁实现<a href=\"#循环神经网络的简洁实现\" title=\"循环神经网络的简洁实现\"></a></h3><h4 id=\"定义模型-1\">定义模型<a href=\"#定义模型-1\" title=\"定义模型\"></a></h4><p>我们使用Pytorch中的<code>nn.RNN</code>来构造循环神经网络。在本节中，我们主要关注<code>nn.RNN</code>的以下几个构造函数参数：</p>\n<ul><li><code>input_size</code> - The number of expected features in the input x</li><li><code>hidden_size</code> – The number of features in the hidden state h</li><li><code>nonlinearity</code> – The non-linearity to use. Can be either &#39;tanh&#39; or &#39;relu&#39;. Default: &#39;tanh&#39;</li><li><code>batch_first</code> – If True, then the input and output tensors are provided as (batch_size, num_steps, input_size). Default: False</li></ul><p>这里的<code>batch_first</code>决定了输入的形状，我们使用默认的参数<code>False</code>，对应的输入形状是 (num_steps, batch_size, input_size)。</p>\n<p><code>forward</code>函数的参数为：</p>\n<ul><li><code>input</code> of shape (num_steps, batch_size, input_size): tensor containing the features of the input sequence.</li><li><code>h_0</code> of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.</li></ul><p><code>forward</code>函数的返回值是：</p>\n<ul><li><code>output</code> of shape (num_steps, batch_size, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.</li><li><code>h_n</code> of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the hidden state for t = num_steps.</li></ul><p>现在我们构造一个<code>nn.RNN</code>实例，并用一个简单的例子来看一下输出的形状。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class=\"line\">num_steps, batch_size = <span class=\"number\">35</span>, <span class=\"number\">2</span></span><br><span class=\"line\">X = torch.rand(num_steps, batch_size, vocab_size)</span><br><span class=\"line\">state = <span class=\"literal\">None</span></span><br><span class=\"line\">Y, state_new = rnn_layer(X, state)</span><br><span class=\"line\">print(Y.shape, state_new.shape)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.Size([35, 2, 256]) torch.Size([1, 2, 256])</span><br></pre></td></tr></table></figure>\n\n<p>我们定义一个完整的基于循环神经网络的语言模型。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">RNNModel</span><span class=\"params\">(nn.Module)</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, rnn_layer, vocab_size)</span>:</span></span><br><span class=\"line\">        super(RNNModel, self).__init__()</span><br><span class=\"line\">        self.rnn = rnn_layer</span><br><span class=\"line\">        self.hidden_size = rnn_layer.hidden_size * (<span class=\"number\">2</span> <span class=\"keyword\">if</span> rnn_layer.bidirectional <span class=\"keyword\">else</span> <span class=\"number\">1</span>) </span><br><span class=\"line\">        self.vocab_size = vocab_size</span><br><span class=\"line\">        self.dense = nn.Linear(self.hidden_size, vocab_size)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span><span class=\"params\">(self, inputs, state)</span>:</span></span><br><span class=\"line\">        <span class=\"comment\"># inputs.shape: (batch_size, num_steps)</span></span><br><span class=\"line\">        X = to_onehot(inputs, vocab_size)</span><br><span class=\"line\">        X = torch.stack(X)  <span class=\"comment\"># X.shape: (num_steps, batch_size, vocab_size)</span></span><br><span class=\"line\">        hiddens, state = self.rnn(X, state)</span><br><span class=\"line\">        hiddens = hiddens.view(<span class=\"number\">-1</span>, hiddens.shape[<span class=\"number\">-1</span>])  <span class=\"comment\"># hiddens.shape: (num_steps * batch_size, hidden_size)</span></span><br><span class=\"line\">        output = self.dense(hiddens)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> output, state</span><br></pre></td></tr></table></figure>\n\n<p>类似的，我们需要实现一个预测函数，与前面的区别在于前向计算和初始化隐藏状态。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict_rnn_pytorch</span><span class=\"params\">(prefix, num_chars, model, vocab_size, device, idx_to_char,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                      char_to_idx)</span>:</span></span><br><span class=\"line\">    state = <span class=\"literal\">None</span></span><br><span class=\"line\">    output = [char_to_idx[prefix[<span class=\"number\">0</span>]]]  <span class=\"comment\"># output记录prefix加上预测的num_chars个字符</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> range(num_chars + len(prefix) - <span class=\"number\">1</span>):</span><br><span class=\"line\">        X = torch.tensor([output[<span class=\"number\">-1</span>]], device=device).view(<span class=\"number\">1</span>, <span class=\"number\">1</span>)</span><br><span class=\"line\">        (Y, state) = model(X, state)  <span class=\"comment\"># 前向计算不需要传入模型参数</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> t &lt; len(prefix) - <span class=\"number\">1</span>:</span><br><span class=\"line\">            output.append(char_to_idx[prefix[t + <span class=\"number\">1</span>]])</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            output.append(Y.argmax(dim=<span class=\"number\">1</span>).item())</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"string\">''</span>.join([idx_to_char[i] <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> output])</span><br></pre></td></tr></table></figure>\n\n<p>接下来实现训练函数，这里只使用了相邻采样。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train_and_predict_rnn_pytorch</span><span class=\"params\">(model, num_hiddens, vocab_size, device,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                                corpus_indices, idx_to_char, char_to_idx,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                                num_epochs, num_steps, lr, clipping_theta,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                                batch_size, pred_period, pred_len, prefixes)</span>:</span></span><br><span class=\"line\">    loss = nn.CrossEntropyLoss()</span><br><span class=\"line\">    optimizer = torch.optim.Adam(model.parameters(), lr=lr)</span><br><span class=\"line\">    model.to(device)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(num_epochs):</span><br><span class=\"line\">        l_sum, n, start = <span class=\"number\">0.0</span>, <span class=\"number\">0</span>, time.time()</span><br><span class=\"line\">        data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) <span class=\"comment\"># 相邻采样</span></span><br><span class=\"line\">        state = <span class=\"literal\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, Y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">            <span class=\"keyword\">if</span> state <span class=\"keyword\">is</span> <span class=\"keyword\">not</span> <span class=\"literal\">None</span>:</span><br><span class=\"line\">                <span class=\"comment\"># 使用detach函数从计算图分离隐藏状态</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> isinstance (state, tuple): <span class=\"comment\"># LSTM, state:(h, c)  </span></span><br><span class=\"line\">                    state[<span class=\"number\">0</span>].detach_()</span><br><span class=\"line\">                    state[<span class=\"number\">1</span>].detach_()</span><br><span class=\"line\">                <span class=\"keyword\">else</span>: </span><br><span class=\"line\">                    state.detach_()</span><br><span class=\"line\">            (output, state) = model(X, state) <span class=\"comment\"># output.shape: (num_steps * batch_size, vocab_size)</span></span><br><span class=\"line\">            y = torch.flatten(Y.T)</span><br><span class=\"line\">            l = loss(output, y.long())</span><br><span class=\"line\">            </span><br><span class=\"line\">            optimizer.zero_grad()</span><br><span class=\"line\">            l.backward()</span><br><span class=\"line\">            grad_clipping(model.parameters(), clipping_theta, device)</span><br><span class=\"line\">            optimizer.step()</span><br><span class=\"line\">            l_sum += l.item() * y.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">            n += y.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">        </span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (epoch + <span class=\"number\">1</span>) % pred_period == <span class=\"number\">0</span>:</span><br><span class=\"line\">            print(<span class=\"string\">'epoch %d, perplexity %f, time %.2f sec'</span> % (</span><br><span class=\"line\">                epoch + <span class=\"number\">1</span>, math.exp(l_sum / n), time.time() - start))</span><br><span class=\"line\">            <span class=\"keyword\">for</span> prefix <span class=\"keyword\">in</span> prefixes:</span><br><span class=\"line\">                print(<span class=\"string\">' -'</span>, predict_rnn_pytorch(</span><br><span class=\"line\">                    prefix, pred_len, model, vocab_size, device, idx_to_char,</span><br><span class=\"line\">                    char_to_idx))</span><br></pre></td></tr></table></figure>\n\n<p>训练模型。</p>\n<figure class=\"highlight\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num_epochs, batch_size, lr, clipping_theta = <span class=\"number\">250</span>, <span class=\"number\">32</span>, <span class=\"number\">1e-3</span>, <span class=\"number\">1e-2</span></span><br><span class=\"line\">pred_period, pred_len, prefixes = <span class=\"number\">50</span>, <span class=\"number\">50</span>, [<span class=\"string\">'分开'</span>, <span class=\"string\">'不分开'</span>]</span><br><span class=\"line\">train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class=\"line\">                            corpus_indices, idx_to_char, char_to_idx,</span><br><span class=\"line\">                            num_epochs, num_steps, lr, clipping_theta,</span><br><span class=\"line\">                            batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure>\n\n<p>结果也不贴了。</p>\n","prev":{"title":"pytorch笔记七：循环神经网络进阶","link":"2020/02/23/pytorch笔记七：循环神经网络进阶"},"next":{"title":"pytorch笔记五：文本预处理","link":"2020/02/16/pytorch笔记五：文本预处理"},"plink":"https://yuyuoo.github.io/2020/02/17/pytorch笔记六：循环神经网络基础/","toc":[{"id":"从零开始实现循环神经网络","title":"从零开始实现循环神经网络","index":"1"},{"id":"循环神经网络的简洁实现","title":"循环神经网络的简洁实现","index":"2"}],"copyright":{"license":"Attribution-NonCommercial-NoDerivatives 4.0 International","published":"February 17, 2020","updated":"March 7, 2020","author":"YuYuoo"}}